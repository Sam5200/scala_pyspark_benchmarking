knitr::opts_chunk$set(echo = TRUE)
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
str(file1)
merged_data = merge(file1,file2)
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
merged_data = merge(file1,file2)
str(file1)
str(file1)
str(merged_data)
str(file1)
str(file2)
str(merged_data)
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
merged_data = rbind(file1,file2)
str(file1)
str(file2)
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file1) = column.names
colnames(file2) = column.names
merged_data = rbind(file1,file2)
str(file1)
str(file2)
str(merged_data)
# str(file1)
# str(file2)
# str(merged_data)
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file1) = column.names
colnames(file2) = column.names
merged_data = rbind(file1,file2)
# str(file1)
# str(file2)
# str(merged_data)
summary(merged_data)
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file1) = column.names
colnames(file2) = column.names
merged_data = rbind(file1,file2)
# str(file1)
# str(file2)
# str(merged_data)
summary(merged_data)
library(tidyverse)
merged_data %>% group_by(Language, Dataset)
merged_data %>%
filter(RunID != 1) %>%
group_by(Language, MachineID, Dataset, Type, Operation) %>%
summarise()
merged_data %>%
filter(RunID != 1) %>%
group_by(Language, MachineID, Dataset, Type, Operation) %>%
summarise(n = n())
merged_data %>%
filter(RunID != 1) %>%
group_by(Language, MachineID, Dataset, Type, Operation) %>%
summarise(n = n(), Mean = mean(TimeTaken), Standard Deviation = sd(TimeTaken))
merged_data %>%
filter(RunID != 1) %>%
group_by(Language, MachineID, Dataset, Type, Operation) %>%
summarise(n = n(), Mean = mean(TimeTaken), Std_dev= sd(TimeTaken))
merged_data %>%
filter(RunID != 1) %>%
group_by(Language, MachineID, Dataset, Type, Operation) %>%
summarise(n = n(), Mean = mean(TimeTaken), Std_dev= sd(TimeTaken), Coeff_Var = Mean/Std_dev)
merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Language, MachineID, Dataset) %>%
summarise(n = n(), Mean = mean(TimeTaken), Std_dev= sd(TimeTaken), Coeff_Var = Mean/Std_dev)
file.size("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file.size("../../Data/Databricks/machine2/dataset_10MB.csv")
file.size("../../Data/Databricks/machine2/dataset_10MB.csv")
file.size("../../Data/Databricks/machine2/dataset_100MB.csv")
file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/1024
file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024)
file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
merged_data %>%
mutate_if(Dataset == "dataset_10MB", size = size_10MB)
merged_data %>%
mutate_if('Dataset' == "dataset_10MB", size = size_10MB)
_
summary(merged_data)
merged_data %>%
mutate_if(Dataset == "dataset_10MB", size = size_10MB)
merged_data %>%
mutate_if(merged_data$Dataset == "dataset_10MB", size = size_10MB)
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB"), Size = c(size_10MB,size_100MB))
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB"), Size = c(size_10MB,size_100MB))
str(size_info)
merged_data %>%
merge(Dataset)
merged_data %>%
merge(size_info, by = Dataset)
merge(size_info, by = "Dataset"")
merged_data %>%
merge(size_info, by = "Dataset")
merged_data %>%
merge(size_info)
merged_data %>%
merge(size_info) %>%
mutate(Throughput = TimeTaken/Size)
merged_data = merged_data %>%
merge(size_info) %>%
mutate(Throughput = TimeTaken/Size)
merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Language, MachineID, Dataset) %>%
summarise(n = n()
,Mean_Time = mean(TimeTaken)
,Std_Dev_Time= sd(TimeTaken)
,Coeff_Var_Time = Mean_Time/Std_Dev_Time
,Mean_Throughput = mean(Throughput)
,Std_Dev_Throughput= sd(Throughput)
,Coeff_Var_Throughput = Mean_Throughput/Std_Dev_Throughput
)
library(DT)
result = merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Language, MachineID, Dataset) %>%
summarise(n = n()
,Mean_Time = mean(TimeTaken)
,Std_Dev_Time= sd(TimeTaken)
,Coeff_Var_Time = Mean_Time/Std_Dev_Time
,Mean_Throughput = mean(Throughput)
,Std_Dev_Throughput= sd(Throughput)
,Coeff_Var_Throughput = Mean_Throughput/Std_Dev_Throughput
)
DT::datatable(result)
knitr::opts_chunk$set(echo = TRUE)
read.clean.files = function(filename){
file = read.csv(filename)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
file1 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
file3 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_500MB_20190323_2332.csv")
# column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
#
# colnames(file1) = column.names
# colnames(file2) = column.names
# colnames(file2) = column.names
merged_data = rbind(file1,file2,file3)
# str(file1)
# str(file2)
# str(merged_data)
summary(merged_data)
str(file1)
str(file3)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
file1 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
file3 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_500MB_20190323_2332.csv")
str(file3)
merged_data = rbind(file1,file2,file3)
summary(merged_data)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
file1 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
file3 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_500MB_20190323_2332.csv")
merged_data = rbind(file1,file2,file3)
summary(merged_data)
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_500MB = file.size("../../Data/Databricks/machine2/dataset_500MB.csv")/(1024*1024)
print(paste("Acrual Size of 10MB file (in MB)",size_10MB))
print(paste("Acrual Size of 100MB file (in MB)",size_100MB))
print(paste("Acrual Size of 500MB file (in MB)",size_500MB))
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB","dataset_500MB")
,Size = c(size_10MB,size_100MB,dataset_500MB))
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_500MB = file.size("../../Data/Databricks/machine2/dataset_500MB.csv")/(1024*1024)
print(paste("Acrual Size of 10MB file (in MB)",size_10MB))
print(paste("Acrual Size of 100MB file (in MB)",size_100MB))
print(paste("Acrual Size of 500MB file (in MB)",size_500MB))
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB","dataset_500MB")
,Size = c(size_10MB,size_100MB,size_500MB))
str(size_info)
merged_data = merged_data %>%
merge(size_info) %>%
mutate(Throughput = TimeTaken/Size)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
file1 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
file3 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_500MB_20190323_2332.csv")
merged_data = rbind(file1,file2,file3)
summary(merged_data)
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_500MB = file.size("../../Data/Databricks/machine2/dataset_500MB.csv")/(1024*1024)
print(paste("Acrual Size of 10MB file (in MB)",size_10MB))
print(paste("Acrual Size of 100MB file (in MB)",size_100MB))
print(paste("Acrual Size of 500MB file (in MB)",size_500MB))
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB","dataset_500MB")
,Size = c(size_10MB,size_100MB,size_500MB))
str(size_info)
merged_data = merged_data %>%
merge(size_info) %>%
mutate(Throughput = TimeTaken/Size)
result = merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Language, MachineID, Dataset) %>%
summarise(n = n()
,Mean_Time = mean(TimeTaken)
,Std_Dev_Time= sd(TimeTaken)
,Coeff_Var_Time = Mean_Time/Std_Dev_Time
,Mean_Throughput = mean(Throughput)
,Std_Dev_Throughput= sd(Throughput)
,Coeff_Var_Throughput = Mean_Throughput/Std_Dev_Throughput
)
DT::datatable(result)
files = list.files(pattern = ".csv$")           # List all .txt files
files
files = list.files(path = "../Results/", pattern = ".csv$")           # List all .txt files
files
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE)           # List all .txt files
files
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE)           # List all .txt files
rows = lapply(files, read.table, header = FALSE) # Read the files into list
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.table, header = FALSE) # Read the files into list
files
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(merged_data) = column.names
head(merged_data)
#merged_data = rbind(file1,file2,file3)
summary(merged_data)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
file1 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")
file3 = read.clean.files("../Results/Databricks/machine2/PySpark_dataset_500MB_20190323_2332.csv")
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(merged_data) = column.names
head(merged_data)
#merged_data = rbind(file1,file2,file3)
summary(merged_data)
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024) # 11.4789848327637
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024) # 115.640992164612
size_500MB = file.size("../../Data/Databricks/machine2/dataset_500MB.csv")/(1024*1024) # 576.678165435791
print(paste("Acrual Size of 10MB file (in MB)",size_10MB))
print(paste("Acrual Size of 100MB file (in MB)",size_100MB))
print(paste("Acrual Size of 500MB file (in MB)",size_500MB))
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB","dataset_500MB")
,Size = c(size_10MB,size_100MB,size_500MB))
str(size_info)
merged_data = merged_data %>%
merge(size_info) %>%
mutate(Throughput = Size/TimeTaken)
summarize_results = function(grouped_data){
rv = grouped_data %>%
summarise(n = n()
,Mean_Time = round(mean(TimeTaken),2)
,Std_Dev_Time= round(sd(TimeTaken),2)
,Coeff_Var_Time = round(Mean_Time/Std_Dev_Time,2)
,Mean_Throughput = round(mean(Throughput),2)
,Std_Dev_Throughput= round(sd(Throughput),2)
,Coeff_Var_Throughput = round(Mean_Throughput/Std_Dev_Throughput,2)
)
return(rv)
}
# Comparison between dataset sizes
group = merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Language, MachineID, Dataset)
result = summarize_results(group)
DT::datatable(result)
# Comparison between Scala and PySpark
group = merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Dataset, MachineID, Language)
result = summarize_results(group)
DT::datatable(result)
grepl("Operations", merged_data$Operation)
sum(grepl("Operations", merged_data$Operation))
gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Operation))
gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Operation)
merged_data(grepl(pattern = "Operations", x = merged_data$Operation),)
merged_data[grepl(pattern = "Operations", x = merged_data$Operation),]
merged_data[grepl(pattern = "Operations", x = merged_data$Type),]
merged_data[grepl(pattern = "Aggregate Operations", x = merged_data$Type),]
merged_data[grepl(pattern = "Aggregate Operation", x = merged_data$Type),]
gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Type)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(merged_data) = column.names
merged_data$Type = gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Type)
#merged_data[grepl(pattern = "Aggregate Operation", x = merged_data$Type),]
head(merged_data)
summary(merged_data)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(merged_data) = column.names
merged_data$Type = as.factor(gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Type))
#merged_data[grepl(pattern = "Aggregate Operation", x = merged_data$Type),]
head(merged_data)
summary(merged_data)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(merged_data) = column.names
merged_data$Type = as.factor(gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Type))
head(merged_data)
summary(merged_data)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
read.clean.files = function(filename){
file = read.csv(filename, header = FALSE)
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(file) = column.names
return(file)
}
files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
rows = lapply(files, read.csv, header = FALSE) # Read the files into list
merged_data = do.call(rbind, rows) # combine the data.frame
column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
colnames(merged_data) = column.names
merged_data$Type = as.factor(gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Type))
head(merged_data)
summary(merged_data)
size_10MB =  11.4789848327637 # file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = 115.640992164612 # file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)
size_200MB = 229.8573
size_300MB = 343.2709
size_500MB = 576.678165435791 # file.size("../../Data/Databricks/machine2/dataset_500MB.csv")/(1024*1024)
print(paste("Actual Size of 10MB file (in MB)",size_10MB))
print(paste("Actual Size of 100MB file (in MB)",size_100MB))
print(paste("Actual Size of 200MB file (in MB)",size_200MB))
print(paste("Actual Size of 300MB file (in MB)",size_300MB))
print(paste("Actual Size of 500MB file (in MB)",size_500MB))
size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB","dataset_200MB","dataset_300MB","dataset_500MB")
,Size = c(size_10MB,size_100MB,size_200MB,size_300MB,size_500MB))
str(size_info)
merged_data = merged_data %>%
merge(size_info) %>%
mutate(Throughput = Size/TimeTaken)
summarize_results = function(grouped_data){
rv = grouped_data %>%
summarise(n = n()
,Mean_Time = round(mean(TimeTaken),2)
,Std_Dev_Time= round(sd(TimeTaken),2)
,Coeff_Var_Time = round(Mean_Time/Std_Dev_Time,2)
,Mean_Throughput = round(mean(Throughput),2)
,Std_Dev_Throughput= round(sd(Throughput),2)
,Coeff_Var_Throughput = round(Mean_Throughput/Std_Dev_Throughput,2)
)
return(rv)
}
# Comparison between dataset sizes
group = merged_data %>%
filter(RunID != 1) %>%
group_by(Type, Operation, Language, MachineID, Dataset)
result = summarize_results(group)
DT::datatable(result)
