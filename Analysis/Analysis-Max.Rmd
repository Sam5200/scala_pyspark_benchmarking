---
title: "Analysis"
author: "Nikhil Gupta"
date: "`r Sys.time()`"
always_allow_html: yes
output:
 html_document:
   toc: true
   toc_float: true
   toc_depth: 3
params:
  options: 0 # 1 for Row, 2 for columns, 3 for Mixed + Aggregate
  targetVar: 'Throughput' #Throughput, TimeTaken
  showOutliers: FALSE 
  
  
---



 
`r htmltools::HTML(paste0('<style type="text/css">div.main-container {max-width:100%;}</style>'))`
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(DT)
library(htmltools)

```

# Setup #


```{r}
options = params$options
targetVar = params$targetVar
targetUnits = ifelse(targetVar=='Throughput','MB/s','secs')
showOutliers = params$showOutliers
outliersQuantile = ifelse(showOutliers, 1 ,0.90)  #use 1.0 to include all outliers
outliersShape = ifelse(showOutliers,  19,NA)  
plotCols=2


```


# Reading Files

```{r}
read.clean.files = function(filename){
  file = read.csv(filename, header = FALSE)
  column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")
  colnames(file) = column.names
  return(file)
}

files = list.files(path = "../Results/", pattern = ".csv$", recursive = TRUE, full.names = TRUE) # List all .csv files
#files

databricks.files = files[grepl("Databricks",files)]
local.vm..files = files[grepl("Local_VM",files)]
cluster.files = files[grepl("Cluster",files)]

rows.databricks = lapply(databricks.files, read.csv, header = FALSE) # Read the files into list
merged.databricks = do.call(rbind, rows.databricks) # combine the data.frame
merged.databricks$Setup = 'Databricks'

rows.local.vm = lapply(local.vm..files, read.csv, header = FALSE) # Read the files into list
merged.local.vm = do.call(rbind, rows.local.vm) # combine the data.frame
merged.local.vm$Setup = 'Local VM'

rows.cluster = lapply(cluster.files, read.csv, header = FALSE) # Read the files into list
merged.cluster = do.call(rbind, rows.cluster) # combine the data.frame
merged.cluster$Setup = 'Cluster'


merged_data = rbind(merged.databricks,merged.local.vm,merged.cluster)
merged_data$Setup = as.factor(merged_data$Setup)

column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken","Setup")
colnames(merged_data) = column.names
merged_data$Type = as.factor(gsub(pattern = "Operations", replacement = "Operation", x = merged_data$Type))

# Convert columns to factors
merged_data$MachineID = as.factor(merged_data$MachineID)
merged_data$Randomize = as.factor(merged_data$Randomize)
merged_data$RunID = as.factor(merged_data$RunID)

merged_data$Dataset = sub("dataset_", "", merged_data$Dataset) 
merged_data$Dataset = sub("MB$", "", merged_data$Dataset) 
merged_data$Dataset = as.factor(merged_data$Dataset)

merged_data$Operation = trimws(as.character(merged_data$Operation),'both')
merged_data[merged_data$Operation =='Mathematical Operations on Columns',]$Operation ='Mathematical Operation on Columns'

merged_data$subType = ''
merged_data[grepl('^Sorting',merged_data$Operation),]$subType = 'Sorting'
merged_data[grepl('^Ranking',merged_data$Operation),]$subType = 'Ranking'
merged_data[grepl('^Split',merged_data$Operation),]$subType = 'Splitting'
merged_data[grepl('^GroupBy',merged_data$Operation),]$subType = 'Grouping'
merged_data[grepl(' Join ',merged_data$Operation),]$subType = 'Joining'
merged_data[grepl('^Merge',merged_data$Operation),]$subType = 'Merging'
merged_data[grepl('^Filter',merged_data$Operation),]$subType = 'Filtering'
merged_data[grepl('^Mathematical',merged_data$Operation),]$subType = 'Mathematics'
merged_data[grepl('^Pivot',merged_data$Operation),]$subType = 'Pivots'
merged_data[grepl('^Running|^Shift',merged_data$Operation),]$subType = 'Run/Shift'
merged_data[grepl('^Writing',merged_data$Operation),]$subType = 'Writing'

```

```{r}
size_10MB =  11.4789848327637 # file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = 115.640992164612 # file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024) 
size_200MB = 229.8573  
size_300MB = 343.2709
size_500MB = 576.678165435791 # file.size("../../Data/Databricks/machine2/dataset_500MB.csv")/(1024*1024) 

print(paste("Actual Size of 10MB file (in MB)",size_10MB))
print(paste("Actual Size of 100MB file (in MB)",size_100MB))
print(paste("Actual Size of 200MB file (in MB)",size_200MB))
print(paste("Actual Size of 300MB file (in MB)",size_300MB))
print(paste("Actual Size of 500MB file (in MB)",size_500MB))

size_info = data.frame(Dataset = c("10","100","200","300","500")
                       ,Size = c(size_10MB,size_100MB,size_200MB,size_300MB,size_500MB))
str(size_info)

merged_data = merged_data %>%
  merge(size_info,by='Dataset') %>%
  mutate(Throughput = Size/TimeTaken)

Setups = c(unique(as.character(merged_data$Setup)))

```

```{r}

data_raw = merged_data
merged_data = merged_data %>% 
    filter(RunID != 1)

str(merged_data)
head(merged_data)
summary(merged_data)

print(!(.Platform$GUI == "RStudio"))
```

# Common Functions
```{r}
subchunkify_clear <- function() {
  files=dir(path='figure/',pattern='sub_chunk_',include.dirs=T,full.names = T)
  all(file.remove(files))
}

subchunkify <- function(g, fig_height=7, fig_width=5, enabled = !(.Platform$GUI == "RStudio")) {
  if (enabled==FALSE) return(plot(g))
  g_deparsed <- paste0(deparse(
    function() {g}
  ), collapse = '')
  
  sub_chunk <- paste0("
  `","``{r sub_chunk_", floor(runif(1) * 10000), ", fig.height=", fig_height, ", fig.width=", fig_width, ", echo=FALSE}",
  "\n(", 
    g_deparsed
    , ")()",
  "\n`","``
  ")
  
  cat(knitr::knit(text = knitr::knit_expand(text = sub_chunk), quiet = TRUE))
}


ggplot_colors = function(plot,strip_angle=0,...){
  plot +
  scale_color_manual(values=c("#ca0020","#0571b0"),breaks=c('PySpark','Scala'))+
  scale_fill_manual(values=c("#f4a582","#92c5de"),breaks=c('PySpark','Scala'))+
    theme_light() +
    ggplot2::theme(strip.text.y=element_text(angle=strip_angle)
                   ,strip.background = element_rect(fill='#636363')
                   ,...)
  }


```

# Simple Linear Regression Model

using a simple LM we can identify the elements that contribute to the speed of the queries, we see Scala  has a coeffiecnet of -7, meaning that it bring an overall benefits of 7 seconds, keeping constant the other variables. 

```{r}
subchunkify_clear()
form=as.formula(paste0(targetVar,' ~ Dataset + Language  + Operation + Setup'))
model = lm(data=merged_data,formula=form)
summary(model)
```

# Comparing of Environments

Comparing Local VM vs. Cloud similar in configuration, controlling by the type operation.  The cloud machine is optimized for Scala and Spark, while the local machine has a standard installation of the softwars. 

We notice  the DataBricks environment benefit the aggregate operation, and that Databricks beenfits the scala encironemtn for the Row Operations.
Anyway, other sitation shows a similar perfomance across lancuages and environment

```{r}
filtered= merged_data 
p=ggplot(data=filtered, aes_string(x='Setup',y=targetVar,color='Language',fill='Language')) + 
    geom_boxplot(outlier.size = 0.1,size=0.1,outlier.shape = outliersShape) +
  scale_y_continuous(limits = c(0, quantile(filtered[[targetVar]],probs=outliersQuantile))
                     ,labels=scales::comma_format(accuracy=1,suffix=targetUnits)) +
   facet_wrap(~Type,scales='free',ncol=plotCols) 
  ggplot_colors(p,axis.text.y=element_text(size=7),legend.position='right')
 
```


# Comparing different data sizes

Comparing the peromance of the differnt queries among different sizes of dataset. We notice that the increased size has an import impact on the time. PySpark is suffering more by the increased size, mainly for Row and Column Operations, while Scala language shows a better performance. 
We can see 

```{r}
plotDataSize=function(filtered,title){
  t = filtered
   p=ggplot(data=t, aes_string(x='Dataset',y=targetVar,color='Language',fill='Language')) + 
     geom_boxplot(outlier.size = 0.1,size=0.1,outlier.shape=outliersShape) +
     scale_y_continuous(limits = c(0, quantile(filtered[[targetVar]],probs=outliersQuantile))
                        ,labels=scales::comma_format(accuracy=1,suffix=targetUnits)) +
     facet_wrap(~Type ,labeller=label_wrap_gen(width=100),scales='free_x',ncol=plotCols)  +
     ggplot2::ggtitle(title)
   
   ggplot_colors(p,strip_angle=0
                 ,axis.text.y=element_text(size=7)
                 ,axis.text.x=element_text(size=9))

}
```

## All Setups

```{r}
filtered= merged_data   

plotDataSize(filtered,'Comparing Datsets - All Environments')
```

## By Setup

```{r}
for (s in Setups) {
  message(s)
  filtered= merged_data %>% filter(Setup==s)
  plot(plotDataSize(filtered,paste0('Comparing Datsets - ',s)))

}



```


# Comparing Query Types

As expected, the slowest queries are the Aggregate one,while the fastest ones are the Row  Operations.Scala outperform PySpark on the row operations, while we don't see a significant differnece on the other query types



```{r fig.height=4, fig.width=8}

filtered=merged_data  
filtered$Type = forcats::fct_reorder(filtered$Type,filtered[[targetVar]],.fun=median,.desc=F)
p=ggplot(data=filtered, aes_string(x='Type',y=targetVar,color='Language',fill='Language')) + 
  geom_boxplot(outlier.size = 0.1,size=0.1,outlier.shape=outliersShape)   +
  scale_y_continuous(limits = c(0, quantile(filtered[[targetVar]],probs=outliersQuantile))
                     ,labels=scales::comma_format(accuracy=1,suffix=targetUnits)) +
  facet_wrap(~Setup,ncol=3) +
  ggplot2::coord_flip() 

ggplot_colors(p,axis.text.x=element_text(size=8)
              ,panel.spacing.x=unit(10,'points')
              ,legend.position='bottom') 
```



# Comparing Queries

```{r}
#functions
plot_get_significant = function(filtered){
    w<-function(oper){wilcox.test(as.formula(paste0(targetVar,' ~ Language ')),alternative='two.sided'
                                  ,filtered,subset = Operation == oper)$p.value}
    ops = unique(filtered$Operation)
    test=sapply(ops,w)
    keep= ops[which(test < 0.025)]
    data2 = filtered[filtered$Operation %in% keep,]
    if(nrow(data2)==0) {message("No Significant Elements") ; return(NULL)}
    return(data2)
}

plot_operations_get = function(filtered,title='',subtitle=''
                               ,ncol = plotCols,facets='Operation',legendpos='right'){
  if(is.null(filtered)) return(ggplot(data=filtered)+geom_blank())
  plotData = filtered %>% mutate(Operation = factor(Operation))
  p=  ggplot(data=plotData, aes_string(x='Dataset',y=targetVar,color='Language')) + 
    ggplot2::stat_summary(aes(group=Language),fun.y=median,geom='line',size=.5 )+
    geom_jitter(alpha=0.5,size=.5,width=0.1)+
    scale_y_continuous(labels=scales::comma_format(accuracy=1,suffix=targetUnits)) +
    facet_wrap(facets,scales='free',ncol=ncol,labeller = label_wrap_gen(width = round(100/ncol))) +
    #facet_grid(Operation~subType,scales='free',labeller = label_wrap_gen()) +
    ggplot2::ggtitle(title,subtitle=subtitle)
  
  ggplot_colors(p,legend.position = legendpos
                ,axis.text.x=element_text(size=8,angle=45,hjust=1)
                ,axis.text.y=element_text(size=8)
                #,strip.background = element_rect(fill='#636363')
                )
}
plot_operations = function(filtered,title,ncol=plotCols,legendpos='right'){
  
  for (st in sort(unique(filtered$subType))){
    data=filter(filtered,subType==st)
    p=plot_operations_get(data,title=paste0(title,' - ' , st),subtitle=' (All Operations) ',ncol=ncol,legendpos=legendpos)
    subchunkify(p, fig_height=ceiling(n_distinct(data$Operation)/ncol)*2.5+.5, fig_width=plotCols*3) 
    
    data=plot_get_significant(filter(filtered,subType==st))
    if(!is.null(data))  {
      p=plot_operations_get(data,title=paste0(title,' - ' , st ),subtitle=' (Significant Operations) ',ncol=ncol)
      subchunkify(p, fig_height=ceiling(n_distinct(data$Operation)/ncol)*2.5+.5, fig_width=ncol*3) 
    }
  }
}

plot_environments = function(filtered,title,ncol=plotCols,legendpos='bottom'){
  for (st in sort(unique(filtered$subType))){
    data=filter(filtered,subType==st)
    p=plot_operations_get(data,title=paste0(title,' - ' , st)
                          ,subtitle=' (All Operations) ',ncol=ncol,facets='Setup'
                          ,legendpos=legendpos)
    subchunkify(p, fig_height=ceiling(n_distinct(data$Setup)/ncol)*2.7+.5, fig_width=ncol*3) 
  }
}
getTable = function(data){
    t=data %>% group_by(subType,Operation,Language) %>%
      summarise_at(.vars=targetVar,.funs=vars(mean)) %>%
      ungroup() %>%
      spread(key=Language,value=targetVar) %>%
      mutate(delta = Scala/PySpark)
    knitr::kable(t,digits=2,col.names=c('Category','Operation','PySpark (MB/sec)','Scala (MB/sec)','Scala/PySpark'),padding=0,format='markdown')
}

```

## Comparing Row operations

```{r echo=FALSE, results='asis'}
#subchunkify
title = 'Rows Operations'
print(htmltools::h4(paste0(title,' - All Environments: ')))
filtered=merged_data %>% filter(Type=='Row Operation') 
print(getTable(filtered))
plot_operations(filtered,title=paste0(title,' - All Environments'),ncol=plotCols)

for(s in Setups) {
  print(htmltools::h4(paste0(title,' - Setup: ',s)))
  print(getTable(filter(filtered,Setup==s)))
  plot_operations(filtered=filter(filtered,Setup==s),title=paste0(title,' - ',s))
} 
```

### by Environment

```{r echo=FALSE, results='asis'}
plot_environments(filtered,title=paste0(title,' - By Environment'),ncol=3)
```


## Comparing Columns operations

```{r echo=FALSE, results='asis'}
title = 'Columns Operations'
print(htmltools::h4(paste0(title,' - All Environments: ')))
filtered=merged_data %>% filter(Type=='Column Operation'
                                ,! Operation %in%  c('Full Outer Join 3 Columns'
                                                      ,'Left Outer Join 3 Columns'
                                                      ,'Inner Join 3 Columns')
)
print(getTable(filtered))
plot_operations(filtered,title=paste0(title,' - All Environments'))
for(s in Setups) {
  print(htmltools::h4(paste0(title,' - Setup:',s)))
  print(getTable(filter(filtered,Setup==s)))
  plot_operations(filtered=filter(filtered,Setup==s),title=paste0(title,' - ',s))
} 
```

### by Environment

```{r echo=FALSE, results='asis'}
plot_environments(filtered,title=paste0(title,' - By Environment'),ncol=3)
```


## Comparing Aggregate operations
Scala is faster than PySpark

```{r echo=FALSE, results='asis'}
title = 'Aggregate Operations'
print(htmltools::h4(paste0(title,' - All Environments: ')))
filtered=merged_data %>% filter(Type=='Aggregate Operation')
print(getTable(filtered))
plot_operations(filtered,title=paste0(title,' - All Environments'))
for(s in Setups) {
  print(htmltools::h4(paste0(title,' - Setup: ',s)))
  print(getTable(filter(filtered,Setup==s)))
  plot_operations(filtered=filter(filtered,Setup==s),title=paste0(title,' - ',s))
} 
```

### by Environment

```{r echo=FALSE, results='asis'}
plot_environments(filtered,title=paste0(title,' - By Environment'),ncol=3)
```


## Comparing Mixed operations
Scala is faster than PySpark

```{r echo=FALSE, results='asis'}
title = 'Mixed Operations'
print(htmltools::h4(paste0(title,' - All Environments: ')))
filtered=merged_data %>% filter(Type=='Mixed Operation')
print(getTable(filtered))
plot_operations(filtered,title=paste0(title,' - All Environments'))
for(s in Setups) {
  print(htmltools::h4(paste0(title,' - Setup: ',s)))
  print(getTable(filter(filtered,Setup==s)))
  plot_operations(filtered=filter(filtered,Setup==s),title=paste0(title,' - ',s))
} 
```

### by Environment

```{r echo=FALSE, results='asis'}
plot_environments(filtered,title=paste0(title,' - By Environment'),ncol=3)
```



# Comparing Runs operations
slight improvement with additonal runs (not statistically significant)

```{r fig.height=3, fig.width=6}
filtered=data_raw 
filtered$Operation = forcats::fct_reorder(filtered$RunID,filtered[[targetVar]],.fun=median,desc=F)
p=ggplot(data=filtered, aes_string(x='RunID',y=targetVar,fill='Language',color='Language')) + 
  geom_boxplot(outlier.size = 0.5,size=0.5,alpha=0.6,outlier.shape = outliersShape)  +
  scale_y_continuous(limits = c(0, quantile(filtered[[targetVar]],probs=outliersQuantile))
                     ,labels=scales::comma_format(accuracy=1,suffix=targetUnits)) 
ggplot_colors(p)
```

## By Setup

```{r fig.height=6, fig.width=6}
p=ggplot(data=filtered, aes_string(x='RunID',y=targetVar,fill='Language',color='Language')) + 
  geom_boxplot(outlier.size = 0.5,size=0.5,alpha=0.6,outlier.shape = outliersShape)  +
  scale_y_continuous(limits = c(0, quantile(filtered[[targetVar]],probs=outliersQuantile))
                     ,labels=scales::comma_format(accuracy=1,suffix=targetUnits))  +
  facet_wrap(~Setup,ncol=1)
ggplot_colors(p)

    
```
 