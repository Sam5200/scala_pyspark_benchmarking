---
title: "Analysis"
author: "Nikhil Gupta"
date: "March 23, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(DT)

```

```{r}
file1 = read.csv("../Results/Databricks/machine2/PySpark_dataset_10MB_20190323_1900.csv")
file2 = read.csv("../Results/Databricks/machine2/PySpark_dataset_100MB_20190323_1935.csv")

column.names = c("Language","Randomize","Dataset","MachineID","RunID","Type","Operation","TimeTaken")

colnames(file1) = column.names
colnames(file2) = column.names  
 
merged_data = rbind(file1,file2)
# str(file1)
# str(file2)
# str(merged_data)

summary(merged_data)
```

```{r}
size_10MB = file.size("../../Data/Databricks/machine2/dataset_10MB.csv")/(1024*1024)
size_100MB = file.size("../../Data/Databricks/machine2/dataset_100MB.csv")/(1024*1024)

size_info = data.frame(Dataset = c("dataset_10MB","dataset_100MB"), Size = c(size_10MB,size_100MB))
str(size_info)
```

```{r}
merged_data = merged_data %>%
  merge(size_info) %>%
  mutate(Throughput = TimeTaken/Size)
```

```{r}
result = merged_data %>% 
  filter(RunID != 1) %>%
  group_by(Type, Operation, Language, MachineID, Dataset) %>%
  summarise(n = n()
            ,Mean_Time = mean(TimeTaken)
            ,Std_Dev_Time= sd(TimeTaken)
            ,Coeff_Var_Time = Mean_Time/Std_Dev_Time
            ,Mean_Throughput = mean(Throughput)
            ,Std_Dev_Throughput= sd(Throughput)
            ,Coeff_Var_Throughput = Mean_Throughput/Std_Dev_Throughput
            )

DT::datatable(result)
```

# Observations
* Throughout (Time/MB) does not remian constant. It decreases with larger datasets