{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PySpark\n",
    "import findspark\n",
    "#findspark.init('/home/max/spark-2.4.0-bin-hadoop2.7')\n",
    "findspark.init('/home/nikhil/spark-2.4.0-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "#sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import (StructField, StructType, StringType, IntegerType, FloatType, DateType)\n",
    "from pyspark.sql.functions import lag, col, concat, lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# For Time Logging\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For error checks\n",
    "import sys\n",
    "\n",
    "# For randomizing queries\n",
    "import random\n",
    "\n",
    "# Print Function name\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc._conf.get('spark.driver.memory')\n",
    "conf = pyspark.SparkConf().setAppName(\"App\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '6G')\n",
    "#        .set('spark.driver.memory', '45G')\n",
    "#        .set('spark.driver.maxResultSize', '10G')\n",
    "        )\n",
    "#sc = SparkContext(conf=conf)\n",
    "sc = pyspark.SparkContext('local',conf=conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "file_loc = \"\"\n",
    "#file_prefix = \"/home/max/SMU-DB7330-Spark_python/blob/datasets/\"\n",
    "file_prefix = \"/home/nikhil/msds7330/datasets/\"\n",
    "file_name = \"dataset_500MB\"\n",
    "\n",
    "debug = 0\n",
    "randomize = 1\n",
    "numBenchmarkRuns = 2 # 5 is the default\n",
    "\n",
    "# 1 for Max\n",
    "# 2 for Nikhil\n",
    "machineID = 2\n",
    "\n",
    "# Total Number of Queries = 36\n",
    "numQueries = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto calculations (DO NOT CHANGE)\n",
    "# Make the directories if they do not exist\n",
    "# dbutils.fs.mkdirs(\"/home/max/SMU-DB7330-Spark_python/blob/timelogs\")\n",
    "# dbutils.fs.mkdirs(\"mnt/blob/datasets\") # Should exist since we have already created the dataset\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "start_of_run  = now.strftime(\"%Y%m%d_%H%M\")\n",
    "#logfilename = \"/home/max/SMU-DB7330-Spark_python/blob/timelogs/PySpark_\" + file_name + \"_\" + start_of_run + \".csv\"\n",
    "logfilename = \"/home/nikhil/msds7330/SMU-DB7330-Spark_python/Results/Local_VM/machine2/PySpark_\" + file_name + \"_\" + start_of_run + \".csv\"\n",
    "logfilename_withDBFS = \"\" + logfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def time_usage(runID, name=\"\"):\n",
    "    \"\"\"log the time usage in a code block\n",
    "    prefix: the prefix text to show\n",
    "    \"\"\"\n",
    "    #print (\"In time_usage runID = {}\".format(runID))\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    elapsed_seconds = float(\"%.10f\" % (end - start))\n",
    "    logging.info('%s: elapsed seconds: %s', name, elapsed_seconds)\n",
    "    output = \"\\n\" + str(runID) + \",\" + name + \",\" + str(elapsed_seconds)\n",
    "       \n",
    "    # https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\n",
    "    # https://stackoverflow.com/questions/49318402/read-write-single-file-in-databricks\n",
    "    with open(logfilename_withDBFS,\"a\") as file:\n",
    "      file.write(\"\\n\" + \"PySpark\" + \",\" \n",
    "                 + str(randomize) + \",\" \n",
    "                 + file_name + \",\" \n",
    "                 + str(machineID) + \",\" \n",
    "                 + str(runID) + \",\" \n",
    "                 + name + \",\" \n",
    "                 + str(elapsed_seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Loading Data\")\n",
    "data = spark.read.csv(file_prefix + file_name + \".csv\", inferSchema = True, header=True)\n",
    "data_join = data.limit(int(data.count()/2))\n",
    "data_100 = spark.read.csv(file_prefix + file_name + \"_add_100.csv\", inferSchema = True, header=True)\n",
    "data_1000 = spark.read.csv(file_prefix + file_name + \"_add_1000.csv\", inferSchema = True, header=True)\n",
    "data_10000 = spark.read.csv(file_prefix + file_name + \"_add_10000.csv\", inferSchema = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "data_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_100.count() != 100:\n",
    "  sys.exit(\"Num Rows Expected was 100. Got something else.\")\n",
    "if data_1000.count() != 1000:\n",
    "  sys.exit(\"Num Rows Expected was 1000. Got something else.\")\n",
    "if data_10000.count() != 10000:\n",
    "  sys.exit(\"Num Rows Expected was 10000. Got something else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jaxenter.com/implement-switch-case-statement-python-138315.html\n",
    "\n",
    "def query1(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # filtering rows based on a condition\n",
    "  with time_usage(runID,\"Row Operation, Filter\"):\n",
    "    temp = data.filter((data['int17'] > 200) & (data['float17'] < 200)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "\n",
    "def query2(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # Filtering rows based on regular expressions\n",
    "  # expr = \"^troubleshot\"\n",
    "  expr = \"^overmeddled\"\n",
    "  with time_usage(runID,\"Row Operation, Filter Reg Ex 1\"):\n",
    "    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()  \n",
    "\n",
    "def query3(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # lf at 3rd and 4th position and ending with w\n",
    "  expr = \".{2}lf.*s$\" \n",
    "  with time_usage(runID,\"Row Operation, Filter Reg Ex 2\"):\n",
    "    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['group0']) # check\n",
    "\n",
    "def query4(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # Shift (Lag) operation\n",
    "  w = Window.orderBy(\"Int1\")\n",
    "  with time_usage(runID,\"Row Operation, Shift (Lag)\"):\n",
    "    temp = data.withColumn('status_lag', lag(col('Int1')).over(w)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[1]['status_lag'])\n",
    "  \n",
    "def query5(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # running sum\n",
    "  # https://stackoverflow.com/questions/46982119/how-to-calculate-cumulative-sum-in-a-pyspark-table\n",
    "  window = Window.orderBy(\"Int1\")\n",
    "  with time_usage(runID,\"Row Operation, Running Sum\"):\n",
    "    temp = data.withColumn(\"CumSumTotal\", sum(data['Int1']).over(window)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[1]['CumSumTotal'])\n",
    "  \n",
    "def query6(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Row Operation, Writing 100 new rows\"):\n",
    "    temp = data.union(data_100).collect()\n",
    "  \n",
    "def query7(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Row Operation, Writing 1000 new rows\"):\n",
    "    data.union(data_1000).collect()  \n",
    "\n",
    "def query8(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Row Operation, Writing 10000 new rows\"):\n",
    "    data.union(data_10000).collect()\n",
    "    \n",
    "def query9(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Asc 1 column\"):\n",
    "    temp = data.orderBy(\"words0\").collect()\n",
    "  temp[0]['words0']\n",
    "  \n",
    "def query10(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Asc 5 column\"):\n",
    "    temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words1'])\n",
    "  \n",
    "def query11(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Asc 10 column\"):\n",
    "      temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\",\"words5\",\"words6\",\"words7\",\"words8\",\"words9\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words2'])\n",
    "  \n",
    "def query12(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Desc 1 column\"):\n",
    "    temp = data.orderBy(data[\"words0\"].desc()).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words0'])\n",
    "  \n",
    "def query13(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Desc 5 column\"):\n",
    "    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc()).collect()\n",
    "  temp[0]['words1']\n",
    "  \n",
    "def query14(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Desc 10 column\"):\n",
    "    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc(),data[\"words5\"].desc(),data[\"words6\"].desc(),data[\"words7\"].desc(),data[\"words8\"].desc(),data[\"words9\"].desc()).collect()\n",
    "  temp[0]['words2'] \n",
    "  \n",
    "def query15(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # aggregate function will apply function to the whole dataframe (not grouping by a column)\n",
    "  # takes a dictionary as input\n",
    "  # however this may not be the best way to do this: https://stackoverflow.com/a/51855775\n",
    "  with time_usage(runID,\"Column Operation, Mathematical Operation on Columns\"):\n",
    "    temp = data.select(['Int1','Float1','Int2','Float3','Float10']).agg({'Int1':'sum','Float1':'avg','Int2':'count','Float3':'min','Float10':'max'}).collect()\n",
    "  temp[0]\n",
    "\n",
    "def query16(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "   \n",
    "  # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n",
    "  if (file_name == \"dataset_10MB\"):\n",
    "    with time_usage(runID,\"Column Operation, Inner Join 3 Columns\"):\n",
    "      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'inner').collect()\n",
    "    if debug >= 1:\n",
    "      print(temp[0]['group0'])\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query17(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Inner Join 5 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'inner').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "\n",
    "def query18(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Inner Join 10 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'inner').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query19(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "    \n",
    "  if (file_name == \"dataset_10MB\"):\n",
    "    with time_usage(runID,\"Column Operation, Left Outer Join 3 Columns\"):\n",
    "      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'left_outer').collect()\n",
    "    if debug >= 1:\n",
    "      print(temp[0]['group1'])\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query20(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Left Outer Join 5 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'left_outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query21(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Left Outer Join 10 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'left_outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query22(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if (file_name == \"dataset_10MB\"):\n",
    "    with time_usage(runID,\"Column Operation, Full Outer Join 3 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2'], how = 'outer').collect()\n",
    "    if debug >= 1:\n",
    "      print(temp[0]['group2'])\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "\n",
    "def query23(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Full Outer Join 5 Columns\"):\n",
    "      data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query24(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Full Outer Join 10 Columns\"):\n",
    "      data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "\n",
    "def query25(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n",
    "  with time_usage(runID,\"Column Operation, Split 1 Column into 5\"):\n",
    "    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n",
    "    split_col.getItem(1)\n",
    "    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n",
    "    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n",
    "    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n",
    "    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n",
    "    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n",
    "    data_new = data_new.collect()\n",
    "  if debug >= 1:\n",
    "    print (data_new[0]['words0_0'])\n",
    "    print (data_new[0]['words0_1'])\n",
    "    print (data_new[0]['words0_2'])\n",
    "\n",
    "def query26(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Split 1 Column into 10\"):\n",
    "    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n",
    "    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n",
    "    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n",
    "    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n",
    "    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n",
    "    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n",
    "    data_new = data_new.withColumn(\"words0_5\", split_col.getItem(5))\n",
    "    data_new = data_new.withColumn(\"words0_6\", split_col.getItem(6))\n",
    "    data_new = data_new.withColumn(\"words0_7\", split_col.getItem(7))\n",
    "    data_new = data_new.withColumn(\"words0_8\", split_col.getItem(8))\n",
    "    data_new = data_new.withColumn(\"words0_9\", split_col.getItem(9))\n",
    "    data_new = data_new.collect()\n",
    "  if debug >= 1:\n",
    "    print (data_new[0]['words0'])\n",
    "    print (data_new[0]['words0_1'])\n",
    "    print (data_new[0]['words0_2'])\n",
    "    print (data_new[0]['words0_3'])\n",
    "    print (data_new[0]['words0_7'])\n",
    "    print (data_new[0]['words0_8'])\n",
    "    print (data_new[0]['words0_9'])\n",
    "  \n",
    "def query27(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # https://www.edureka.co/community/2280/concatenate-columns-in-apache-spark-dataframe\n",
    "  with time_usage(runID,\"Column Operation, Merge 2 columns into 1\"):\n",
    "    temp = data.withColumn(\"words0m1\", concat(col(\"words0\")  , lit(\" \"), col(\"words1\") )).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words0m1'])\n",
    "\n",
    "def query28(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Merge 5 columns into 1\"):\n",
    "    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n",
    "                                       ,col(\"words1\"), lit(\" \")\n",
    "                                       ,col(\"words2\"), lit(\" \")\n",
    "                                       ,col(\"words3\"), lit(\" \")\n",
    "                                       ,col(\"words4\")\n",
    "                                      )\n",
    "                   ).collect()\n",
    "\n",
    "def query29(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Merge 10 columns into 1\"):\n",
    "    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n",
    "                                       ,col(\"words1\"), lit(\" \")\n",
    "                                       ,col(\"words2\"), lit(\" \")\n",
    "                                       ,col(\"words3\"), lit(\" \")\n",
    "                                       ,col(\"words4\"), lit(\" \")\n",
    "                                       ,col(\"words5\"), lit(\" \")\n",
    "                                       ,col(\"words6\"), lit(\" \")\n",
    "                                       ,col(\"words7\"), lit(\" \")\n",
    "                                       ,col(\"words8\"), lit(\" \")\n",
    "                                       ,col(\"words9\")\n",
    "                                      )\n",
    "                   ).collect()\n",
    "  \n",
    "def query30(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy (1 group)\n",
    "  with time_usage(runID,\"Aggregate Operation, GroupBy 1 column\"):\n",
    "    temp1 = data.groupBy(\"Group0\").count().collect()\n",
    "    temp2 = data.groupBy(\"Group0\").sum().collect()\n",
    "    temp3 = data.groupBy(\"Group0\").avg().collect()\n",
    "    temp4 = data.groupBy(\"Group0\").min().collect()\n",
    "    temp5 = data.groupBy(\"Group0\").max().collect()\n",
    "  if debug >= 1:\n",
    "    print(temp1[0])\n",
    "    print(temp2[0])\n",
    "    print(temp3[0])\n",
    "    print(temp4[0])\n",
    "    print(temp5[0])\n",
    "\n",
    "\n",
    "def query31(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy (5 groups)\n",
    "  with time_usage(runID,\"Aggregate Operation, GroupBy 5 columns\"):\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').count().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').sum().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').avg().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').min().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').max().collect()\n",
    "\n",
    "def query32(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "   #GroupBy (10 groups)\n",
    "  with time_usage(runID,\"Aggregate Operation, GroupBy 10 columns\"):\n",
    "     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').count().collect()\n",
    "     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').sum().collect()\n",
    "     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').avg().collect()\n",
    "     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').min().collect()\n",
    "     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').max().collect()\n",
    "\n",
    "    \n",
    "def query33(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy with ranking\n",
    "  # https://stackoverflow.com/a/41662162\n",
    "  with time_usage(runID,\"Aggregate Operation, Ranking by Group\"):\n",
    "    temp = data.withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"Group0\").orderBy(desc(\"Int1\")))).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0][\"rank\"])\n",
    "    print(temp[0][\"group0\"])\n",
    "    print(temp[0][\"int1\"])  \n",
    "  \n",
    "def query34(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # Scala Syntax is identical to PySpark\n",
    "  # df.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").count()\n",
    "  with time_usage(runID,\"Mixed Operation, Pivot 1 Rows and 1 Column\"):\n",
    "    temp = data.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "    \n",
    "def query35(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Mixed Operation, Pivot 5 Rows and 1 Column\"):\n",
    "    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\").pivot(\"group10\").sum(\"float1\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "  \n",
    "def query36(runID, debug = 0):\n",
    "    print(inspect.stack()[0][3]) # Prints function name\n",
    "    with time_usage(runID,\"Mixed Operation, Pivot 10 Rows and 1 Column\"):\n",
    "        temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\",\"group5\",\"group6\",\"group7\",\"group8\",\"group9\").pivot(\"group10\").sum(\"float2\").collect()\n",
    "    if debug >= 1:\n",
    "        print(temp[0])\n",
    "\n",
    "def unexpected(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  print(\"Unexpected case\")\n",
    "\n",
    "switcher = {\n",
    "  1: query1,\n",
    "  2: query2,\n",
    "  3: query3,\n",
    "  4: query4,\n",
    "  5: query5,\n",
    "  6: query6,\n",
    "  7: query7,\n",
    "  8: query8,\n",
    "  9: query9,\n",
    "  10: query10,\n",
    "  11: query11,\n",
    "  12: query12,\n",
    "  13: query13,\n",
    "  14: query14,\n",
    "  15: query15,\n",
    "  16: query16,\n",
    "  17: query17,\n",
    "  18: query18,\n",
    "  19: query19,\n",
    "  20: query20,\n",
    "  21: query21,\n",
    "  22: query22,\n",
    "  23: query23,\n",
    "  24: query24,\n",
    "  25: query25,\n",
    "  26: query26,\n",
    "  27: query27,\n",
    "  28: query28,\n",
    "  29: query29,\n",
    "  30: query30,\n",
    "  31: query31,\n",
    "  32: query32,\n",
    "  33: query33,\n",
    "  34: query34,\n",
    "  35: query35,\n",
    "  36: query36,\n",
    "  \"whoa\": unexpected\n",
    "}\n",
    " \n",
    "def run_function(argument, runID, debug = 0):\n",
    "    func = switcher.get(argument) # gets just the name of the function from switcher\n",
    "    return func(runID, debug) # returns the call to the function\n",
    "  \n",
    "def runQueries(runID = 1, randomize = 0, debug = 0):\n",
    "    qList = list(range(1,numQueries+1)) # python is non inclusive of last number\n",
    "    print (\"---------------------------------\")\n",
    "    print (\"Run ID: {}\".format(runID))\n",
    "    print (\"---------------------------------\")\n",
    "  \n",
    "    if(randomize == 1):\n",
    "        seed_val = runID * 100\n",
    "        random.seed(seed_val)\n",
    "        random.shuffle(qList)\n",
    "    for queryNum in qList:\n",
    "        run_function(queryNum, runID, debug)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify numBenchmarkRuns + 2 here since we discard 1st run and python is not inclusive of last number\n",
    "for runID in range(1,numBenchmarkRuns+2):\n",
    "    runQueries(runID = runID,randomize = randomize, debug = debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('Language',StringType(),True) \n",
    "              ,StructField('Randomize',IntegerType(),True) \n",
    "              ,StructField('Dataset',StringType(),True)\n",
    "              ,StructField('MachineID',IntegerType(),True) \n",
    "              ,StructField('RunID',IntegerType(),True)\n",
    "              ,StructField('Type',StringType(),True) \n",
    "              ,StructField('Operation',StringType(),True)\n",
    "              ,StructField('TimeTaken',FloatType(),True)]\n",
    "\n",
    "final_struct = StructType(fields = data_schema)\n",
    "\n",
    "print(logfilename)\n",
    "timelog = spark.read.csv(logfilename, schema = final_struct)\n",
    "logging.getLogger().setLevel(logging.WARNING) # supress all informational items\n",
    "timelog.show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays all files in a location\n",
    "# https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\n",
    "# display(dbutils.fs.ls(\"dbfs:/mnt/blob/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"dbfs:/mnt/blob/datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"dbfs:/mnt/blob/timelogs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETING FILES (USE WITH CARE)\n",
    "#dbutils.fs.rm(\"/mnt/blob/timelogs/PySpark_dataset_500MB_20190323_2323.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving a File\n",
    "# https://forums.databricks.com/questions/14312/how-to-move-files-of-same-extension-in-databricks.html\n",
    "# dbutils.fs.mv(\"dbfs:/mnt/blob/add_100.csv\", \"dbfs:/mnt/blob/datasets/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Open with\")\n",
    "# print(\"https://community.cloud.databricks.com/dbfs\" + logfilename + \"?o=6744756749927366\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "Project_PySpark_v2",
  "notebookId": 1662278327812399
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
