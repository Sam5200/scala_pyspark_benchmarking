{"cells":[{"cell_type":"code","source":["# For PySpark\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql.types import (StructField, StructType, StringType, IntegerType, FloatType, DateType)\nfrom pyspark.sql.functions import lag, col, concat, lit\nfrom pyspark.sql.functions import *\n\n# For Time Logging\nfrom contextlib import contextmanager\nimport logging\nimport time\nimport datetime\n\n# For error checks\nimport sys\n\n# For randomizing queries\nimport random\n\n# Print Function name\nimport inspect"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Setup\nfile_loc = \"/dbfs\"\nfile_prefix = \"/mnt/blob/datasets/\"\nfile_name = \"dataset_200MB\"\n\ndebug = 0\nrandomize = 1\nnumBenchmarkRuns = 5 # 5 is the default\n\n# 1 for Max\n# 2 for Nikhil\nmachineID = 2\n\n# Total Number of Queries = 36\nnumQueries = 36"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Auto calculations (DO NOT CHANGE)\n# Make the directories if they do not exist\ndbutils.fs.mkdirs(\"mnt/blob/timelogs\")\n# dbutils.fs.mkdirs(\"mnt/blob/datasets\") # Should exist since we have already created the dataset\n\nnow = datetime.datetime.now()\nstart_of_run  = now.strftime(\"%Y%m%d_%H%M\")\nlogfilename = \"/mnt/blob/timelogs/PySpark_\" + file_name + \"_\" + start_of_run + \".csv\"\nlogfilename_withDBFS = \"/dbfs\" + logfilename"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["@contextmanager\ndef time_usage(runID, name=\"\"):\n    \"\"\"log the time usage in a code block\n    prefix: the prefix text to show\n    \"\"\"\n    #print (\"In time_usage runID = {}\".format(runID))\n    start = time.time()\n    yield\n    end = time.time()\n    elapsed_seconds = float(\"%.10f\" % (end - start))\n    logging.info('%s: elapsed seconds: %s', name, elapsed_seconds)\n    output = \"\\n\" + str(runID) + \",\" + name + \",\" + str(elapsed_seconds)\n       \n    # https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\n    # https://stackoverflow.com/questions/49318402/read-write-single-file-in-databricks\n    with open(logfilename_withDBFS,\"a\") as file:\n      file.write(\"\\n\" + \"PySpark\" + \",\" \n                 + str(randomize) + \",\" \n                 + file_name + \",\" \n                 + str(machineID) + \",\" \n                 + str(runID) + \",\" \n                 + name + \",\" \n                 + str(elapsed_seconds))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#logging.getLogger().setLevel(logging.INFO)\nlogging.getLogger().setLevel(logging.WARNING)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["print (\"Loading Data\")\ndata = spark.read.csv(file_prefix + file_name + \".csv\", inferSchema = True, header=True)\ndata_join = data.limit(int(data.count()/2))\ndata_100 = spark.read.csv(file_prefix + file_name + \"_add_100.csv\", inferSchema = True, header=True)\ndata_1000 = spark.read.csv(file_prefix + file_name + \"_add_1000.csv\", inferSchema = True, header=True)\ndata_10000 = spark.read.csv(file_prefix + file_name + \"_add_10000.csv\", inferSchema = True, header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading Data\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# testing\ndata_join.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>100000\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["if data_100.count() != 100:\n  sys.exit(\"Num Rows Expected was 100. Got something else.\")\nif data_1000.count() != 1000:\n  sys.exit(\"Num Rows Expected was 1000. Got something else.\")\nif data_10000.count() != 10000:\n  sys.exit(\"Num Rows Expected was 10000. Got something else.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["data.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n[&apos;float0&apos;,\n &apos;float1&apos;,\n &apos;float2&apos;,\n &apos;float3&apos;,\n &apos;float4&apos;,\n &apos;float5&apos;,\n &apos;float6&apos;,\n &apos;float7&apos;,\n &apos;float8&apos;,\n &apos;float9&apos;,\n &apos;float10&apos;,\n &apos;float11&apos;,\n &apos;float12&apos;,\n &apos;float13&apos;,\n &apos;float14&apos;,\n &apos;float15&apos;,\n &apos;float16&apos;,\n &apos;float17&apos;,\n &apos;float18&apos;,\n &apos;float19&apos;,\n &apos;int0&apos;,\n &apos;int1&apos;,\n &apos;int2&apos;,\n &apos;int3&apos;,\n &apos;int4&apos;,\n &apos;int5&apos;,\n &apos;int6&apos;,\n &apos;int7&apos;,\n &apos;int8&apos;,\n &apos;int9&apos;,\n &apos;int10&apos;,\n &apos;int11&apos;,\n &apos;int12&apos;,\n &apos;int13&apos;,\n &apos;int14&apos;,\n &apos;int15&apos;,\n &apos;int16&apos;,\n &apos;int17&apos;,\n &apos;int18&apos;,\n &apos;int19&apos;,\n &apos;words0&apos;,\n &apos;words1&apos;,\n &apos;words2&apos;,\n &apos;words3&apos;,\n &apos;words4&apos;,\n &apos;words5&apos;,\n &apos;words6&apos;,\n &apos;words7&apos;,\n &apos;words8&apos;,\n &apos;words9&apos;,\n &apos;group0&apos;,\n &apos;group1&apos;,\n &apos;group2&apos;,\n &apos;group3&apos;,\n &apos;group4&apos;,\n &apos;group5&apos;,\n &apos;group6&apos;,\n &apos;group7&apos;,\n &apos;group8&apos;,\n &apos;group9&apos;,\n &apos;group10&apos;]\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# https://jaxenter.com/implement-switch-case-statement-python-138315.html\n\ndef query1(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # filtering rows based on a condition\n  with time_usage(runID,\"Row Operation, Filter\"):\n    temp = data.filter((data['int17'] > 200) & (data['float17'] < 200)).collect()\n  if debug >= 1:\n    print(temp[0])\n\ndef query2(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # Filtering rows based on regular expressions\n  # expr = \"^troubleshot\"\n  expr = \"^overmeddled\"\n  with time_usage(runID,\"Row Operation, Filter Reg Ex 1\"):\n    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()  \n\ndef query3(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # lf at 3rd and 4th position and ending with w\n  expr = \".{2}lf.*s$\" \n  with time_usage(runID,\"Row Operation, Filter Reg Ex 2\"):\n    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()\n  if debug >= 1:\n    print(temp[0]['group0']) # check\n\ndef query4(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # Shift (Lag) operation\n  w = Window.orderBy(\"Int1\")\n  with time_usage(runID,\"Row Operation, Shift (Lag)\"):\n    temp = data.withColumn('status_lag', lag(col('Int1')).over(w)).collect()\n  if debug >= 1:\n    print(temp[1]['status_lag'])\n  \ndef query5(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # running sum\n  # https://stackoverflow.com/questions/46982119/how-to-calculate-cumulative-sum-in-a-pyspark-table\n  window = Window.orderBy(\"Int1\")\n  with time_usage(runID,\"Row Operation, Running Sum\"):\n    temp = data.withColumn(\"CumSumTotal\", sum(data['Int1']).over(window)).collect()\n  if debug >= 1:\n    print(temp[1]['CumSumTotal'])\n  \ndef query6(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Row Operation, Writing 100 new rows\"):\n    temp = data.union(data_100).collect()\n  \ndef query7(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Row Operation, Writing 1000 new rows\"):\n    data.union(data_1000).collect()  \n\ndef query8(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Row Operation, Writing 10000 new rows\"):\n    data.union(data_10000).collect()\n    \ndef query9(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Asc 1 column\"):\n    temp = data.orderBy(\"words0\").collect()\n  temp[0]['words0']\n  \ndef query10(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Asc 5 column\"):\n    temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\").collect()\n  if debug >= 1:\n    print(temp[0]['words1'])\n  \ndef query11(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Asc 10 column\"):\n      temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\",\"words5\",\"words6\",\"words7\",\"words8\",\"words9\").collect()\n  if debug >= 1:\n    print(temp[0]['words2'])\n  \ndef query12(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Desc 1 column\"):\n    temp = data.orderBy(data[\"words0\"].desc()).collect()\n  if debug >= 1:\n    print(temp[0]['words0'])\n  \ndef query13(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Desc 5 column\"):\n    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc()).collect()\n  temp[0]['words1']\n  \ndef query14(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Desc 10 column\"):\n    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc(),data[\"words5\"].desc(),data[\"words6\"].desc(),data[\"words7\"].desc(),data[\"words8\"].desc(),data[\"words9\"].desc()).collect()\n  temp[0]['words2'] \n  \ndef query15(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # aggregate function will apply function to the whole dataframe (not grouping by a column)\n  # takes a dictionary as input\n  # however this may not be the best way to do this: https://stackoverflow.com/a/51855775\n  with time_usage(runID,\"Column Operation, Mathematical Operation on Columns\"):\n    temp = data.select(['Int1','Float1','Int2','Float3','Float10']).agg({'Int1':'sum','Float1':'avg','Int2':'count','Float3':'min','Float10':'max'}).collect()\n  temp[0]\n\ndef query16(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n   \n  # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n  if (file_name == \"dataset_10MB\"):\n    with time_usage(runID,\"Column Operation, Inner Join 3 Columns\"):\n      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'inner').collect()\n    if debug >= 1:\n      print(temp[0]['group0'])\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query17(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Inner Join 5 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'inner').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n\ndef query18(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Inner Join 10 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'inner').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query19(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n    \n  if (file_name == \"dataset_10MB\"):\n    with time_usage(runID,\"Column Operation, Left Outer Join 3 Columns\"):\n      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'left_outer').collect()\n    if debug >= 1:\n      print(temp[0]['group1'])\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query20(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Left Outer Join 5 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'left_outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query21(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Left Outer Join 10 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'left_outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query22(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if (file_name == \"dataset_10MB\"):\n    with time_usage(runID,\"Column Operation, Full Outer Join 3 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2'], how = 'outer').collect()\n    if debug >= 1:\n      print(temp[0]['group2'])\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n\ndef query23(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Full Outer Join 5 Columns\"):\n      data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query24(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Full Outer Join 10 Columns\"):\n      data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n\ndef query25(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n  with time_usage(runID,\"Column Operation, Split 1 Column into 5\"):\n    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n    split_col.getItem(1)\n    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n    data_new = data_new.collect()\n  if debug >= 1:\n    print (data_new[0]['words0_0'])\n    print (data_new[0]['words0_1'])\n    print (data_new[0]['words0_2'])\n\ndef query26(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Split 1 Column into 10\"):\n    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n    data_new = data_new.withColumn(\"words0_5\", split_col.getItem(5))\n    data_new = data_new.withColumn(\"words0_6\", split_col.getItem(6))\n    data_new = data_new.withColumn(\"words0_7\", split_col.getItem(7))\n    data_new = data_new.withColumn(\"words0_8\", split_col.getItem(8))\n    data_new = data_new.withColumn(\"words0_9\", split_col.getItem(9))\n    data_new = data_new.collect()\n  if debug >= 1:\n    print (data_new[0]['words0'])\n    print (data_new[0]['words0_1'])\n    print (data_new[0]['words0_2'])\n    print (data_new[0]['words0_3'])\n    print (data_new[0]['words0_7'])\n    print (data_new[0]['words0_8'])\n    print (data_new[0]['words0_9'])\n  \ndef query27(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # https://www.edureka.co/community/2280/concatenate-columns-in-apache-spark-dataframe\n  with time_usage(runID,\"Column Operation, Merge 2 columns into 1\"):\n    temp = data.withColumn(\"words0m1\", concat(col(\"words0\")  , lit(\" \"), col(\"words1\") )).collect()\n  if debug >= 1:\n    print(temp[0]['words0m1'])\n\ndef query28(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Merge 5 columns into 1\"):\n    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n                                       ,col(\"words1\"), lit(\" \")\n                                       ,col(\"words2\"), lit(\" \")\n                                       ,col(\"words3\"), lit(\" \")\n                                       ,col(\"words4\")\n                                      )\n                   ).collect()\n\ndef query29(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Merge 10 columns into 1\"):\n    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n                                       ,col(\"words1\"), lit(\" \")\n                                       ,col(\"words2\"), lit(\" \")\n                                       ,col(\"words3\"), lit(\" \")\n                                       ,col(\"words4\"), lit(\" \")\n                                       ,col(\"words5\"), lit(\" \")\n                                       ,col(\"words6\"), lit(\" \")\n                                       ,col(\"words7\"), lit(\" \")\n                                       ,col(\"words8\"), lit(\" \")\n                                       ,col(\"words9\")\n                                      )\n                   ).collect()\n  \ndef query30(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy (1 group)\n  with time_usage(runID,\"Aggregate Operation, GroupBy 1 column\"):\n    temp1 = data.groupBy(\"Group0\").count().collect()\n    temp2 = data.groupBy(\"Group0\").sum().collect()\n    temp3 = data.groupBy(\"Group0\").avg().collect()\n    temp4 = data.groupBy(\"Group0\").min().collect()\n    temp5 = data.groupBy(\"Group0\").max().collect()\n  if debug >= 1:\n    print(temp1[0])\n    print(temp2[0])\n    print(temp3[0])\n    print(temp4[0])\n    print(temp5[0])\n\n\ndef query31(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy (5 groups)\n  with time_usage(runID,\"Aggregate Operation, GroupBy 5 columns\"):\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').count().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').sum().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').avg().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').min().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').max().collect()\n\ndef query32(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy (10 groups)\n  with time_usage(runID,\"Aggregate Operation, GroupBy 10 columns\"):\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').count().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').sum().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').avg().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').min().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').max().collect()\n\n    \ndef query33(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy with ranking\n  # https://stackoverflow.com/a/41662162\n  with time_usage(runID,\"Aggregate Operation, Ranking by Group\"):\n    temp = data.withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"Group0\").orderBy(desc(\"Int1\")))).collect()\n  if debug >= 1:\n    print(temp[0][\"rank\"])\n    print(temp[0][\"group0\"])\n    print(temp[0][\"int1\"])  \n  \ndef query34(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # Scala Syntax is identical to PySpark\n  # df.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").count()\n  with time_usage(runID,\"Mixed Operation, Pivot 1 Rows and 1 Column\"):\n    temp = data.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").collect()\n  if debug >= 1:\n    print(temp[0])\n    \ndef query35(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Mixed Operation, Pivot 5 Rows and 1 Column\"):\n    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\").pivot(\"group10\").sum(\"float1\").collect()\n  if debug >= 1:\n    print(temp[0])\n  \ndef query36(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Mixed Operation, Pivot 10 Rows and 1 Column\"):\n    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\",\"group5\",\"group6\",\"group7\",\"group8\",\"group9\").pivot(\"group10\").sum(\"float2\").collect()\n  if debug >= 1:\n    print(temp[0])\n\ndef unexpected(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  print(\"Unexpected case\")\n\nswitcher = {\n  1: query1,\n  2: query2,\n  3: query3,\n  4: query4,\n  5: query5,\n  6: query6,\n  7: query7,\n  8: query8,\n  9: query9,\n  10: query10,\n  11: query11,\n  12: query12,\n  13: query13,\n  14: query14,\n  15: query15,\n  16: query16,\n  17: query17,\n  18: query18,\n  19: query19,\n  20: query20,\n  21: query21,\n  22: query22,\n  23: query23,\n  24: query24,\n  25: query25,\n  26: query26,\n  27: query27,\n  28: query28,\n  29: query29,\n  30: query30,\n  31: query31,\n  32: query32,\n  33: query33,\n  34: query34,\n  35: query35,\n  36: query36,\n  \"whoa\": unexpected\n}\n \ndef run_function(argument, runID, debug = 0):\n    func = switcher.get(argument) # gets just the name of the function from switcher\n    return func(runID, debug) # returns the call to the function\n  \ndef runQueries(runID = 1, randomize = 0, debug = 0):\n  qList = list(range(1,numQueries+1)) # python is non inclusive of last number\n  print (\"---------------------------------\")\n  print (\"Run ID: {}\".format(runID))\n  print (\"---------------------------------\")\n  \n  if(randomize == 1):\n    seed_val = runID * 100\n    random.seed(seed_val)\n    random.shuffle(qList)\n  for queryNum in qList:\n    run_function(queryNum, runID, debug)   \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# specify numBenchmarkRuns + 2 here since we discard 1st run and python is not inclusive of last number\nfor runID in range(1,numBenchmarkRuns+2):\n  runQueries(runID = runID,randomize = randomize, debug = debug)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">---------------------------------\nRun ID: 1\n---------------------------------\nquery1\nquery25\nquery5\nquery19\nSkipping this query as it gives memory error for larger datasets\nquery20\nSkipping this query as it gives memory error for larger datasets\nquery16\nSkipping this query as it gives memory error for larger datasets\nquery28\nquery21\nSkipping this query as it gives memory error for larger datasets\nquery6\nquery22\nSkipping this query as it gives memory error for larger datasets\nquery34\nquery23\nSkipping this query as it gives memory error for larger datasets\nquery31\nquery13\nquery29\nquery36\nquery8\nquery11\nquery7\nquery2\nquery9\nquery15\nquery3\nquery27\nquery18\nSkipping this query as it gives memory error for larger datasets\nquery4\nquery32\nquery17\nSkipping this query as it gives memory error for larger datasets\nquery14\nquery33\nquery24\nSkipping this query as it gives memory error for larger datasets\nquery26\nquery12\nquery35\nquery30\nquery10\n---------------------------------\nRun ID: 2\n---------------------------------\nquery22\nSkipping this query as it gives memory error for larger datasets\nquery24\nSkipping this query as it gives memory error for larger datasets\nquery5\nquery36\nquery17\nSkipping this query as it gives memory error for larger datasets\nquery35\nquery16\nSkipping this query as it gives memory error for larger datasets\nquery13\nquery19\nSkipping this query as it gives memory error for larger datasets\nquery34\nquery11\nquery26\nquery32\nquery20\nSkipping this query as it gives memory error for larger datasets\nquery12\nquery33\nquery4\nquery7\nquery8\nquery21\nSkipping this query as it gives memory error for larger datasets\nquery25\nquery9\nquery28\nquery31\nquery27\nquery6\nquery29\nquery15\nquery23\nSkipping this query as it gives memory error for larger datasets\nquery1\nquery30\nquery18\nSkipping this query as it gives memory error for larger datasets\nquery10\nquery2\nquery14\nquery3\n---------------------------------\nRun ID: 3\n---------------------------------\nquery16\nSkipping this query as it gives memory error for larger datasets\nquery21\nSkipping this query as it gives memory error for larger datasets\nquery7\nquery13\nquery31\nquery24\nSkipping this query as it gives memory error for larger datasets\nquery3\nquery18\nSkipping this query as it gives memory error for larger datasets\nquery2\nquery34\nquery20\nSkipping this query as it gives memory error for larger datasets\nquery9\nquery10\nquery22\nSkipping this query as it gives memory error for larger datasets\nquery15\nquery33\nquery8\nquery5\nquery26\nquery12\nquery11\nquery28\nquery4\nquery25\nquery6\nquery30\nquery19\nSkipping this query as it gives memory error for larger datasets\nquery14\nquery29\nquery27\nquery17\nSkipping this query as it gives memory error for larger datasets\nquery35\nquery32\nquery1\nquery36\nquery23\nSkipping this query as it gives memory error for larger datasets\n---------------------------------\nRun ID: 4\n---------------------------------\nquery12\nquery11\nquery26\nquery34\nquery23\nSkipping this query as it gives memory error for larger datasets\nquery35\nquery3\nquery29\nquery21\nSkipping this query as it gives memory error for larger datasets\nquery19\nSkipping this query as it gives memory error for larger datasets\nquery16\nSkipping this query as it gives memory error for larger datasets\nquery22\nSkipping this query as it gives memory error for larger datasets\nquery4\nquery24\nSkipping this query as it gives memory error for larger datasets\nquery36\nquery13\nquery1\nquery8\nquery2\nquery5\nquery27\nquery7\nquery15\nquery10\nquery9\nquery18\nSkipping this query as it gives memory error for larger datasets\nquery31\nquery28\nquery25\nquery33\nquery14\nquery30\nquery32\nquery6\nquery17\nSkipping this query as it gives memory error for larger datasets\nquery20\nSkipping this query as it gives memory error for larger datasets\n---------------------------------\nRun ID: 5\n---------------------------------\nquery23\nSkipping this query as it gives memory error for larger datasets\nquery12\nquery29\nquery21\nSkipping this query as it gives memory error for larger datasets\nquery27\nquery9\nquery22\nSkipping this query as it gives memory error for larger datasets\nquery33\nquery14\nquery16\nSkipping this query as it gives memory error for larger datasets\nquery15\nquery5\nquery18\nSkipping this query as it gives memory error for larger datasets\nquery20\nSkipping this query as it gives memory error for larger datasets\nquery6\nquery32\nquery2\nquery10\nquery13\nquery8\nquery28\nquery31\nquery1\nquery3\nquery24\nSkipping this query as it gives memory error for larger datasets\nquery19\nSkipping this query as it gives memory error for larger datasets\nquery35\nquery11\nquery4\nquery26\nquery7\nquery25\nquery17\nSkipping this query as it gives memory error for larger datasets\nquery36\nquery34\nquery30\n---------------------------------\nRun ID: 6\n---------------------------------\nquery36\nquery34\nquery8\nquery5\nquery33\nquery19\nSkipping this query as it gives memory error for larger datasets\nquery24\nSkipping this query as it gives memory error for larger datasets\nquery21\nSkipping this query as it gives memory error for larger datasets\nquery9\nquery32\nquery16\nSkipping this query as it gives memory error for larger datasets\nquery28\nquery20\nSkipping this query as it gives memory error for larger datasets\nquery25\nquery27\nquery26\nquery6\nquery4\nquery11\nquery12\nquery30\nquery1\nquery22\nSkipping this query as it gives memory error for larger datasets\nquery18\nSkipping this query as it gives memory error for larger datasets\nquery3\nquery35\nquery2\nquery14\nquery15\nquery31\nquery13\nquery7\nquery10\nquery29\nquery23\nSkipping this query as it gives memory error for larger datasets\nquery17\nSkipping this query as it gives memory error for larger datasets\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["data_schema = [StructField('Language',StringType(),True) \n              ,StructField('Randomize',IntegerType(),True) \n              ,StructField('Dataset',StringType(),True)\n              ,StructField('MachineID',IntegerType(),True) \n              ,StructField('RunID',IntegerType(),True)\n              ,StructField('Type',StringType(),True) \n              ,StructField('Operation',StringType(),True)\n              ,StructField('TimeTaken',FloatType(),True)]\n\nfinal_struct = StructType(fields = data_schema)\n\nprint(logfilename)\ntimelog = spark.read.csv(logfilename, schema = final_struct)\nlogging.getLogger().setLevel(logging.WARNING) # supress all informational items\ntimelog.show(10000)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/blob/timelogs/PySpark_dataset_200MB_20190327_0204.csv\n+--------+---------+-------------+---------+-----+-------------------+--------------------+----------+\nLanguage|Randomize|      Dataset|MachineID|RunID|               Type|           Operation| TimeTaken|\n+--------+---------+-------------+---------+-----+-------------------+--------------------+----------+\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation|              Filter|  4.652553|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Split 1 Column i...| 11.766997|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation|         Running Sum|  21.62674|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Merge 5 columns ...| 12.135535|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation| Writing 100 new ...| 10.018933|\n PySpark|        1|dataset_200MB|        2|    1|    Mixed Operation| Pivot 1 Rows and...| 7.2433767|\n PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|   GroupBy 5 columns|  37.67363|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Desc 5 c...|  60.96046|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Merge 10 columns...|  12.08664|\n PySpark|        1|dataset_200MB|        2|    1|    Mixed Operation| Pivot 10 Rows an...| 16.581488|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation| Writing 10000 ne...|   16.6762|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Asc 10 c...|  49.60384|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation| Writing 1000 new...| 10.504343|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation|     Filter Reg Ex 1| 2.6379268|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Asc 1 co...| 48.144398|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Mathematical Ope...| 1.5376029|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation|     Filter Reg Ex 2| 2.8180661|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Merge 2 columns ...| 13.313459|\n PySpark|        1|dataset_200MB|        2|    1|      Row Operation|         Shift (Lag)| 15.894673|\n PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|  GroupBy 10 columns| 51.195763|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Desc 10 ...| 32.560555|\n PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|    Ranking by Group| 13.500582|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Split 1 Column i...| 13.547101|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Desc 1 c...| 42.490295|\n PySpark|        1|dataset_200MB|        2|    1|    Mixed Operation| Pivot 5 Rows and...| 12.300534|\n PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|    GroupBy 1 column|  18.97193|\n PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Asc 5 co...|  51.04864|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation|         Running Sum| 12.951963|\n PySpark|        1|dataset_200MB|        2|    2|    Mixed Operation| Pivot 10 Rows an...| 14.890925|\n PySpark|        1|dataset_200MB|        2|    2|    Mixed Operation| Pivot 5 Rows and...| 13.132951|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Desc 5 c...|   50.8436|\n PySpark|        1|dataset_200MB|        2|    2|    Mixed Operation| Pivot 1 Rows and...|  4.913946|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Asc 10 c...|    66.548|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Split 1 Column i...| 12.993703|\n PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|  GroupBy 10 columns|  48.24642|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Desc 1 c...| 63.871933|\n PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|    Ranking by Group| 12.241798|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation|         Shift (Lag)| 15.150602|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation| Writing 1000 new...|  9.472681|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation| Writing 10000 ne...| 12.540118|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Split 1 Column i...|  13.99193|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Asc 1 co...| 36.726906|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Merge 5 columns ...| 13.188966|\n PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|   GroupBy 5 columns| 32.697876|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Merge 2 columns ...| 10.108783|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation| Writing 100 new ...|  9.384319|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Merge 10 columns...|  15.41621|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Mathematical Ope...|  1.790127|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation|              Filter|     3.818|\n PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|    GroupBy 1 column| 17.441383|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Asc 5 co...| 42.837223|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation|     Filter Reg Ex 1| 2.4659255|\n PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Desc 10 ...| 44.827015|\n PySpark|        1|dataset_200MB|        2|    2|      Row Operation|     Filter Reg Ex 2| 2.5977216|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation| Writing 1000 new...| 11.373891|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Desc 5 c...|  48.61929|\n PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|   GroupBy 5 columns| 30.702564|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation|     Filter Reg Ex 2| 2.5244195|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation|     Filter Reg Ex 1| 2.9178982|\n PySpark|        1|dataset_200MB|        2|    3|    Mixed Operation| Pivot 1 Rows and...| 4.6772213|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Asc 1 co...| 40.202076|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Asc 5 co...|  68.24492|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Mathematical Ope...| 1.4702477|\n PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|    Ranking by Group| 12.814879|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation| Writing 10000 ne...| 11.759948|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation|         Running Sum| 16.353033|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Split 1 Column i...|  15.00292|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Desc 1 c...|  38.45776|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Asc 10 c...|  41.22801|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Merge 5 columns ...| 14.163449|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation|         Shift (Lag)|  20.64348|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Split 1 Column i...| 12.753967|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation| Writing 100 new ...|10.1632595|\n PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|    GroupBy 1 column| 18.233896|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Desc 10 ...|   60.7628|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Merge 10 columns...| 14.033956|\n PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Merge 2 columns ...|14.6378565|\n PySpark|        1|dataset_200MB|        2|    3|    Mixed Operation| Pivot 5 Rows and...|12.7107525|\n PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|  GroupBy 10 columns| 48.964245|\n PySpark|        1|dataset_200MB|        2|    3|      Row Operation|              Filter| 3.6591258|\n PySpark|        1|dataset_200MB|        2|    3|    Mixed Operation| Pivot 10 Rows an...| 13.787422|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Desc 1 c...|  66.44919|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Asc 10 c...| 50.456165|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Split 1 Column i...| 16.947489|\n PySpark|        1|dataset_200MB|        2|    4|    Mixed Operation| Pivot 1 Rows and...|  5.032236|\n PySpark|        1|dataset_200MB|        2|    4|    Mixed Operation| Pivot 5 Rows and...| 11.582377|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation|     Filter Reg Ex 2| 2.6108155|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Merge 10 columns...| 13.094477|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation|         Shift (Lag)| 17.706583|\n PySpark|        1|dataset_200MB|        2|    4|    Mixed Operation| Pivot 10 Rows an...| 14.068739|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Desc 5 c...|  39.48249|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation|              Filter|  6.260732|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation| Writing 10000 ne...| 17.976023|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation|     Filter Reg Ex 1| 3.2467313|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation|         Running Sum| 17.831646|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Merge 2 columns ...|13.8633795|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation| Writing 1000 new...|  8.709115|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Mathematical Ope...| 1.5214925|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Asc 5 co...| 25.357311|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Asc 1 co...| 24.158466|\n PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|   GroupBy 5 columns|  35.22343|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Merge 5 columns ...| 12.937048|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Split 1 Column i...|  21.15601|\n PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|    Ranking by Group|  28.56262|\n PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Desc 10 ...| 50.058784|\n PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|    GroupBy 1 column| 21.157274|\n PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|  GroupBy 10 columns|  46.28201|\n PySpark|        1|dataset_200MB|        2|    4|      Row Operation| Writing 100 new ...|  10.12348|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Desc 1 c...| 56.924442|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Merge 10 columns...| 17.753628|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Merge 2 columns ...| 13.721421|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Asc 1 co...|  45.83067|\n PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|    Ranking by Group| 12.706296|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Desc 10 ...|  38.33555|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Mathematical Ope...| 1.4980462|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation|         Running Sum| 17.857409|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation| Writing 100 new ...| 11.425949|\n PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|  GroupBy 10 columns|   50.6067|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation|     Filter Reg Ex 1|  2.546941|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Asc 5 co...|  58.41099|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Desc 5 c...|  93.48637|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation| Writing 10000 ne...| 14.819243|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Merge 5 columns ...|  16.85726|\n PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|   GroupBy 5 columns|  69.88675|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation|              Filter|  7.709294|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation|     Filter Reg Ex 2| 5.0929585|\n PySpark|        1|dataset_200MB|        2|    5|    Mixed Operation| Pivot 5 Rows and...| 15.124113|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Asc 10 c...|  52.40623|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation|         Shift (Lag)| 15.689404|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Split 1 Column i...| 13.004327|\n PySpark|        1|dataset_200MB|        2|    5|      Row Operation| Writing 1000 new...|12.6085415|\n PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Split 1 Column i...| 13.583167|\n PySpark|        1|dataset_200MB|        2|    5|    Mixed Operation| Pivot 10 Rows an...| 14.068534|\n PySpark|        1|dataset_200MB|        2|    5|    Mixed Operation| Pivot 1 Rows and...| 4.6140976|\n PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|    GroupBy 1 column| 18.561369|\n PySpark|        1|dataset_200MB|        2|    6|    Mixed Operation| Pivot 10 Rows an...| 13.586413|\n PySpark|        1|dataset_200MB|        2|    6|    Mixed Operation| Pivot 1 Rows and...|  4.985581|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation| Writing 10000 ne...| 14.288491|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation|         Running Sum|   15.5333|\n PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|    Ranking by Group|  16.38609|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Asc 1 co...| 39.986095|\n PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|  GroupBy 10 columns| 71.580795|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Merge 5 columns ...|  11.18101|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Split 1 Column i...| 14.317528|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Merge 2 columns ...| 12.640671|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Split 1 Column i...| 12.883596|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation| Writing 100 new ...|  9.747298|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation|         Shift (Lag)| 20.916378|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Asc 10 c...|  58.30399|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Desc 1 c...| 66.805405|\n PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|    GroupBy 1 column|  17.41619|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation|              Filter| 3.4869204|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation|     Filter Reg Ex 2| 2.3455906|\n PySpark|        1|dataset_200MB|        2|    6|    Mixed Operation| Pivot 5 Rows and...| 12.038018|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation|     Filter Reg Ex 1| 2.9779055|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Desc 10 ...|  73.92443|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Mathematical Ope...| 1.4425983|\n PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|   GroupBy 5 columns| 32.986137|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Desc 5 c...|  73.54623|\n PySpark|        1|dataset_200MB|        2|    6|      Row Operation| Writing 1000 new...| 12.545465|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Asc 5 co...| 83.954605|\n PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Merge 10 columns...|  16.22411|\n+--------+---------+-------------+---------+-----+-------------------+--------------------+----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Displays all files in a location\n# https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\ndisplay(dbutils.fs.ls(\"dbfs:/mnt/blob/\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/datasets/</td><td>datasets/</td><td>0</td></tr><tr><td>dbfs:/mnt/blob/timelogs/</td><td>timelogs/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/mnt/blob/datasets\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB.csv</td><td>dataset_100MB.csv</td><td>121258369</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_100.csv</td><td>dataset_100MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_1000.csv</td><td>dataset_100MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_10000.csv</td><td>dataset_100MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB.csv</td><td>dataset_10MB.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_100.csv</td><td>dataset_10MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_1000.csv</td><td>dataset_10MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_10000.csv</td><td>dataset_10MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB.csv</td><td>dataset_1GB.csv</td><td>1204049059</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB_add_100.csv</td><td>dataset_1GB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB_add_1000.csv</td><td>dataset_1GB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB_add_10000.csv</td><td>dataset_1GB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB.csv</td><td>dataset_200MB.csv</td><td>241022871</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_100.csv</td><td>dataset_200MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_1000.csv</td><td>dataset_200MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_10000.csv</td><td>dataset_200MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB.csv</td><td>dataset_300MB.csv</td><td>359945718</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_100.csv</td><td>dataset_300MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_1000.csv</td><td>dataset_300MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_10000.csv</td><td>dataset_300MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB.csv</td><td>dataset_500MB.csv</td><td>604690884</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_100.csv</td><td>dataset_500MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_1000.csv</td><td>dataset_500MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_10000.csv</td><td>dataset_500MB_add_10000.csv</td><td>12036588</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/mnt/blob/timelogs\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_100MB_20190323_1935.csv</td><td>PySpark_dataset_100MB_20190323_1935.csv</td><td>15886</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_10MB_20190323_1900.csv</td><td>PySpark_dataset_10MB_20190323_1900.csv</td><td>17117</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_200MB_20190327_0204.csv</td><td>PySpark_dataset_200MB_20190327_0204.csv</td><td>12940</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_300MB_20190326_2328.csv</td><td>PySpark_dataset_300MB_20190326_2328.csv</td><td>12945</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_500MB_20190323_2332.csv</td><td>PySpark_dataset_500MB_20190323_2332.csv</td><td>12976</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_100MB_20190383_0610.csv</td><td>time_Scala_Random_dataset_100MB_20190383_0610.csv</td><td>14332</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_10MB_20190383_0554.csv</td><td>time_Scala_Random_dataset_10MB_20190383_0554.csv</td><td>15421</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190385_1025.csv</td><td>time_Scala_Random_dataset_200MB_20190385_1025.csv</td><td>11629</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_300MB_20190385_0916.csv</td><td>time_Scala_Random_dataset_300MB_20190385_0916.csv</td><td>11680</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_0406.csv</td><td>time_Scala_Random_dataset_500MB_20190386_0406.csv</td><td>7794</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_1205.csv</td><td>time_Scala_Random_dataset_500MB_20190386_1205.csv</td><td>5842</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# DELETING FILES (USE WITH CARE)\n# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_0314.csv\")\n# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_0557.csv\")\n# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190384_1209.csv\")\n# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190384_0427.csv\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Moving a File\n# https://forums.databricks.com/questions/14312/how-to-move-files-of-same-extension-in-databricks.html\n# dbutils.fs.mv(\"dbfs:/mnt/blob/add_100.csv\", \"dbfs:/mnt/blob/datasets/.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17}],"metadata":{"name":"Project_PySpark_v2","notebookId":4088465828014280},"nbformat":4,"nbformat_minor":0}
