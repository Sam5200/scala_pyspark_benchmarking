{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For PySpark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import (StructField, StructType, StringType, IntegerType, FloatType, DateType)\n",
    "from pyspark.sql.functions import lag, col, concat, lit\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# For Time Logging\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For error checks\n",
    "import sys\n",
    "\n",
    "# For randomizing queries\n",
    "import random\n",
    "\n",
    "# Print Function name\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "file_loc = \"/dbfs\"\n",
    "file_prefix = \"/mnt/blob/datasets/\"\n",
    "file_name = \"dataset_200MB\"\n",
    "\n",
    "debug = 0\n",
    "randomize = 1\n",
    "numBenchmarkRuns = 5 # 5 is the default\n",
    "\n",
    "# 1 for Max\n",
    "# 2 for Nikhil\n",
    "machineID = 2\n",
    "\n",
    "# Total Number of Queries = 36\n",
    "numQueries = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Auto calculations (DO NOT CHANGE)\n",
    "# Make the directories if they do not exist\n",
    "dbutils.fs.mkdirs(\"mnt/blob/timelogs\")\n",
    "# dbutils.fs.mkdirs(\"mnt/blob/datasets\") # Should exist since we have already created the dataset\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "start_of_run  = now.strftime(\"%Y%m%d_%H%M\")\n",
    "logfilename = \"/mnt/blob/timelogs/PySpark_\" + file_name + \"_\" + start_of_run + \".csv\"\n",
    "logfilename_withDBFS = \"/dbfs\" + logfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@contextmanager\n",
    "def time_usage(runID, name=\"\"):\n",
    "    \"\"\"log the time usage in a code block\n",
    "    prefix: the prefix text to show\n",
    "    \"\"\"\n",
    "    #print (\"In time_usage runID = {}\".format(runID))\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    elapsed_seconds = float(\"%.10f\" % (end - start))\n",
    "    logging.info('%s: elapsed seconds: %s', name, elapsed_seconds)\n",
    "    output = \"\\n\" + str(runID) + \",\" + name + \",\" + str(elapsed_seconds)\n",
    "       \n",
    "    # https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\n",
    "    # https://stackoverflow.com/questions/49318402/read-write-single-file-in-databricks\n",
    "    with open(logfilename_withDBFS,\"a\") as file:\n",
    "      file.write(\"\\n\" + \"PySpark\" + \",\" \n",
    "                 + str(randomize) + \",\" \n",
    "                 + file_name + \",\" \n",
    "                 + str(machineID) + \",\" \n",
    "                 + str(runID) + \",\" \n",
    "                 + name + \",\" \n",
    "                 + str(elapsed_seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Loading Data\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Loading Data\")\n",
    "data = spark.read.csv(file_prefix + file_name + \".csv\", inferSchema = True, header=True)\n",
    "data_join = data.limit(int(data.count()/2))\n",
    "data_100 = spark.read.csv(file_prefix + file_name + \"_add_100.csv\", inferSchema = True, header=True)\n",
    "data_1000 = spark.read.csv(file_prefix + file_name + \"_add_1000.csv\", inferSchema = True, header=True)\n",
    "data_10000 = spark.read.csv(file_prefix + file_name + \"_add_10000.csv\", inferSchema = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>100000\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing\n",
    "data_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if data_100.count() != 100:\n",
    "  sys.exit(\"Num Rows Expected was 100. Got something else.\")\n",
    "if data_1000.count() != 1000:\n",
    "  sys.exit(\"Num Rows Expected was 1000. Got something else.\")\n",
    "if data_10000.count() != 10000:\n",
    "  sys.exit(\"Num Rows Expected was 10000. Got something else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n",
       "[&apos;float0&apos;,\n",
       " &apos;float1&apos;,\n",
       " &apos;float2&apos;,\n",
       " &apos;float3&apos;,\n",
       " &apos;float4&apos;,\n",
       " &apos;float5&apos;,\n",
       " &apos;float6&apos;,\n",
       " &apos;float7&apos;,\n",
       " &apos;float8&apos;,\n",
       " &apos;float9&apos;,\n",
       " &apos;float10&apos;,\n",
       " &apos;float11&apos;,\n",
       " &apos;float12&apos;,\n",
       " &apos;float13&apos;,\n",
       " &apos;float14&apos;,\n",
       " &apos;float15&apos;,\n",
       " &apos;float16&apos;,\n",
       " &apos;float17&apos;,\n",
       " &apos;float18&apos;,\n",
       " &apos;float19&apos;,\n",
       " &apos;int0&apos;,\n",
       " &apos;int1&apos;,\n",
       " &apos;int2&apos;,\n",
       " &apos;int3&apos;,\n",
       " &apos;int4&apos;,\n",
       " &apos;int5&apos;,\n",
       " &apos;int6&apos;,\n",
       " &apos;int7&apos;,\n",
       " &apos;int8&apos;,\n",
       " &apos;int9&apos;,\n",
       " &apos;int10&apos;,\n",
       " &apos;int11&apos;,\n",
       " &apos;int12&apos;,\n",
       " &apos;int13&apos;,\n",
       " &apos;int14&apos;,\n",
       " &apos;int15&apos;,\n",
       " &apos;int16&apos;,\n",
       " &apos;int17&apos;,\n",
       " &apos;int18&apos;,\n",
       " &apos;int19&apos;,\n",
       " &apos;words0&apos;,\n",
       " &apos;words1&apos;,\n",
       " &apos;words2&apos;,\n",
       " &apos;words3&apos;,\n",
       " &apos;words4&apos;,\n",
       " &apos;words5&apos;,\n",
       " &apos;words6&apos;,\n",
       " &apos;words7&apos;,\n",
       " &apos;words8&apos;,\n",
       " &apos;words9&apos;,\n",
       " &apos;group0&apos;,\n",
       " &apos;group1&apos;,\n",
       " &apos;group2&apos;,\n",
       " &apos;group3&apos;,\n",
       " &apos;group4&apos;,\n",
       " &apos;group5&apos;,\n",
       " &apos;group6&apos;,\n",
       " &apos;group7&apos;,\n",
       " &apos;group8&apos;,\n",
       " &apos;group9&apos;,\n",
       " &apos;group10&apos;]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://jaxenter.com/implement-switch-case-statement-python-138315.html\n",
    "\n",
    "def query1(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # filtering rows based on a condition\n",
    "  with time_usage(runID,\"Row Operation, Filter\"):\n",
    "    temp = data.filter((data['int17'] > 200) & (data['float17'] < 200)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "\n",
    "def query2(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # Filtering rows based on regular expressions\n",
    "  # expr = \"^troubleshot\"\n",
    "  expr = \"^overmeddled\"\n",
    "  with time_usage(runID,\"Row Operation, Filter Reg Ex 1\"):\n",
    "    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()  \n",
    "\n",
    "def query3(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # lf at 3rd and 4th position and ending with w\n",
    "  expr = \".{2}lf.*s$\" \n",
    "  with time_usage(runID,\"Row Operation, Filter Reg Ex 2\"):\n",
    "    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['group0']) # check\n",
    "\n",
    "def query4(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # Shift (Lag) operation\n",
    "  w = Window.orderBy(\"Int1\")\n",
    "  with time_usage(runID,\"Row Operation, Shift (Lag)\"):\n",
    "    temp = data.withColumn('status_lag', lag(col('Int1')).over(w)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[1]['status_lag'])\n",
    "  \n",
    "def query5(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # running sum\n",
    "  # https://stackoverflow.com/questions/46982119/how-to-calculate-cumulative-sum-in-a-pyspark-table\n",
    "  window = Window.orderBy(\"Int1\")\n",
    "  with time_usage(runID,\"Row Operation, Running Sum\"):\n",
    "    temp = data.withColumn(\"CumSumTotal\", sum(data['Int1']).over(window)).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[1]['CumSumTotal'])\n",
    "  \n",
    "def query6(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Row Operation, Writing 100 new rows\"):\n",
    "    temp = data.union(data_100).collect()\n",
    "  \n",
    "def query7(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Row Operation, Writing 1000 new rows\"):\n",
    "    data.union(data_1000).collect()  \n",
    "\n",
    "def query8(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Row Operation, Writing 10000 new rows\"):\n",
    "    data.union(data_10000).collect()\n",
    "    \n",
    "def query9(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Asc 1 column\"):\n",
    "    temp = data.orderBy(\"words0\").collect()\n",
    "  temp[0]['words0']\n",
    "  \n",
    "def query10(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Asc 5 column\"):\n",
    "    temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words1'])\n",
    "  \n",
    "def query11(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Asc 10 column\"):\n",
    "      temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\",\"words5\",\"words6\",\"words7\",\"words8\",\"words9\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words2'])\n",
    "  \n",
    "def query12(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Desc 1 column\"):\n",
    "    temp = data.orderBy(data[\"words0\"].desc()).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words0'])\n",
    "  \n",
    "def query13(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Desc 5 column\"):\n",
    "    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc()).collect()\n",
    "  temp[0]['words1']\n",
    "  \n",
    "def query14(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Sorting Desc 10 column\"):\n",
    "    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc(),data[\"words5\"].desc(),data[\"words6\"].desc(),data[\"words7\"].desc(),data[\"words8\"].desc(),data[\"words9\"].desc()).collect()\n",
    "  temp[0]['words2'] \n",
    "  \n",
    "def query15(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # aggregate function will apply function to the whole dataframe (not grouping by a column)\n",
    "  # takes a dictionary as input\n",
    "  # however this may not be the best way to do this: https://stackoverflow.com/a/51855775\n",
    "  with time_usage(runID,\"Column Operation, Mathematical Operation on Columns\"):\n",
    "    temp = data.select(['Int1','Float1','Int2','Float3','Float10']).agg({'Int1':'sum','Float1':'avg','Int2':'count','Float3':'min','Float10':'max'}).collect()\n",
    "  temp[0]\n",
    "\n",
    "def query16(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "   \n",
    "  # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n",
    "  if (file_name == \"dataset_10MB\"):\n",
    "    with time_usage(runID,\"Column Operation, Inner Join 3 Columns\"):\n",
    "      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'inner').collect()\n",
    "    if debug >= 1:\n",
    "      print(temp[0]['group0'])\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query17(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Inner Join 5 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'inner').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "\n",
    "def query18(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Inner Join 10 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'inner').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query19(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "    \n",
    "  if (file_name == \"dataset_10MB\"):\n",
    "    with time_usage(runID,\"Column Operation, Left Outer Join 3 Columns\"):\n",
    "      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'left_outer').collect()\n",
    "    if debug >= 1:\n",
    "      print(temp[0]['group1'])\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query20(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Left Outer Join 5 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'left_outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query21(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Left Outer Join 10 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'left_outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query22(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if (file_name == \"dataset_10MB\"):\n",
    "    with time_usage(runID,\"Column Operation, Full Outer Join 3 Columns\"):\n",
    "      temp = data.join(data_join, ['group0','group1','group2'], how = 'outer').collect()\n",
    "    if debug >= 1:\n",
    "      print(temp[0]['group2'])\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "\n",
    "def query23(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  \n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Full Outer Join 5 Columns\"):\n",
    "      data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "    \n",
    "def query24(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n",
    "    with time_usage(runID,\"Column Operation, Full Outer Join 10 Columns\"):\n",
    "      data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'outer').collect()\n",
    "  else:\n",
    "    print (\"Skipping this query as it gives memory error for larger datasets\")\n",
    "\n",
    "def query25(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n",
    "  with time_usage(runID,\"Column Operation, Split 1 Column into 5\"):\n",
    "    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n",
    "    split_col.getItem(1)\n",
    "    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n",
    "    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n",
    "    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n",
    "    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n",
    "    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n",
    "    data_new = data_new.collect()\n",
    "  if debug >= 1:\n",
    "    print (data_new[0]['words0_0'])\n",
    "    print (data_new[0]['words0_1'])\n",
    "    print (data_new[0]['words0_2'])\n",
    "\n",
    "def query26(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Split 1 Column into 10\"):\n",
    "    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n",
    "    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n",
    "    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n",
    "    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n",
    "    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n",
    "    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n",
    "    data_new = data_new.withColumn(\"words0_5\", split_col.getItem(5))\n",
    "    data_new = data_new.withColumn(\"words0_6\", split_col.getItem(6))\n",
    "    data_new = data_new.withColumn(\"words0_7\", split_col.getItem(7))\n",
    "    data_new = data_new.withColumn(\"words0_8\", split_col.getItem(8))\n",
    "    data_new = data_new.withColumn(\"words0_9\", split_col.getItem(9))\n",
    "    data_new = data_new.collect()\n",
    "  if debug >= 1:\n",
    "    print (data_new[0]['words0'])\n",
    "    print (data_new[0]['words0_1'])\n",
    "    print (data_new[0]['words0_2'])\n",
    "    print (data_new[0]['words0_3'])\n",
    "    print (data_new[0]['words0_7'])\n",
    "    print (data_new[0]['words0_8'])\n",
    "    print (data_new[0]['words0_9'])\n",
    "  \n",
    "def query27(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # https://www.edureka.co/community/2280/concatenate-columns-in-apache-spark-dataframe\n",
    "  with time_usage(runID,\"Column Operation, Merge 2 columns into 1\"):\n",
    "    temp = data.withColumn(\"words0m1\", concat(col(\"words0\")  , lit(\" \"), col(\"words1\") )).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0]['words0m1'])\n",
    "\n",
    "def query28(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Merge 5 columns into 1\"):\n",
    "    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n",
    "                                       ,col(\"words1\"), lit(\" \")\n",
    "                                       ,col(\"words2\"), lit(\" \")\n",
    "                                       ,col(\"words3\"), lit(\" \")\n",
    "                                       ,col(\"words4\")\n",
    "                                      )\n",
    "                   ).collect()\n",
    "\n",
    "def query29(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Column Operation, Merge 10 columns into 1\"):\n",
    "    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n",
    "                                       ,col(\"words1\"), lit(\" \")\n",
    "                                       ,col(\"words2\"), lit(\" \")\n",
    "                                       ,col(\"words3\"), lit(\" \")\n",
    "                                       ,col(\"words4\"), lit(\" \")\n",
    "                                       ,col(\"words5\"), lit(\" \")\n",
    "                                       ,col(\"words6\"), lit(\" \")\n",
    "                                       ,col(\"words7\"), lit(\" \")\n",
    "                                       ,col(\"words8\"), lit(\" \")\n",
    "                                       ,col(\"words9\")\n",
    "                                      )\n",
    "                   ).collect()\n",
    "  \n",
    "def query30(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy (1 group)\n",
    "  with time_usage(runID,\"Aggregate Operation, GroupBy 1 column\"):\n",
    "    temp1 = data.groupBy(\"Group0\").count().collect()\n",
    "    temp2 = data.groupBy(\"Group0\").sum().collect()\n",
    "    temp3 = data.groupBy(\"Group0\").avg().collect()\n",
    "    temp4 = data.groupBy(\"Group0\").min().collect()\n",
    "    temp5 = data.groupBy(\"Group0\").max().collect()\n",
    "  if debug >= 1:\n",
    "    print(temp1[0])\n",
    "    print(temp2[0])\n",
    "    print(temp3[0])\n",
    "    print(temp4[0])\n",
    "    print(temp5[0])\n",
    "\n",
    "\n",
    "def query31(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy (5 groups)\n",
    "  with time_usage(runID,\"Aggregate Operation, GroupBy 5 columns\"):\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').count().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').sum().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').avg().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').min().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').max().collect()\n",
    "\n",
    "def query32(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy (10 groups)\n",
    "  with time_usage(runID,\"Aggregate Operation, GroupBy 10 columns\"):\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').count().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').sum().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').avg().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').min().collect()\n",
    "    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').max().collect()\n",
    "\n",
    "    \n",
    "def query33(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # GroupBy with ranking\n",
    "  # https://stackoverflow.com/a/41662162\n",
    "  with time_usage(runID,\"Aggregate Operation, Ranking by Group\"):\n",
    "    temp = data.withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"Group0\").orderBy(desc(\"Int1\")))).collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0][\"rank\"])\n",
    "    print(temp[0][\"group0\"])\n",
    "    print(temp[0][\"int1\"])  \n",
    "  \n",
    "def query34(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  # Scala Syntax is identical to PySpark\n",
    "  # df.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").count()\n",
    "  with time_usage(runID,\"Mixed Operation, Pivot 1 Rows and 1 Column\"):\n",
    "    temp = data.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "    \n",
    "def query35(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Mixed Operation, Pivot 5 Rows and 1 Column\"):\n",
    "    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\").pivot(\"group10\").sum(\"float1\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "  \n",
    "def query36(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  with time_usage(runID,\"Mixed Operation, Pivot 10 Rows and 1 Column\"):\n",
    "    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\",\"group5\",\"group6\",\"group7\",\"group8\",\"group9\").pivot(\"group10\").sum(\"float2\").collect()\n",
    "  if debug >= 1:\n",
    "    print(temp[0])\n",
    "\n",
    "def unexpected(runID, debug = 0):\n",
    "  print(inspect.stack()[0][3]) # Prints function name\n",
    "  print(\"Unexpected case\")\n",
    "\n",
    "switcher = {\n",
    "  1: query1,\n",
    "  2: query2,\n",
    "  3: query3,\n",
    "  4: query4,\n",
    "  5: query5,\n",
    "  6: query6,\n",
    "  7: query7,\n",
    "  8: query8,\n",
    "  9: query9,\n",
    "  10: query10,\n",
    "  11: query11,\n",
    "  12: query12,\n",
    "  13: query13,\n",
    "  14: query14,\n",
    "  15: query15,\n",
    "  16: query16,\n",
    "  17: query17,\n",
    "  18: query18,\n",
    "  19: query19,\n",
    "  20: query20,\n",
    "  21: query21,\n",
    "  22: query22,\n",
    "  23: query23,\n",
    "  24: query24,\n",
    "  25: query25,\n",
    "  26: query26,\n",
    "  27: query27,\n",
    "  28: query28,\n",
    "  29: query29,\n",
    "  30: query30,\n",
    "  31: query31,\n",
    "  32: query32,\n",
    "  33: query33,\n",
    "  34: query34,\n",
    "  35: query35,\n",
    "  36: query36,\n",
    "  \"whoa\": unexpected\n",
    "}\n",
    " \n",
    "def run_function(argument, runID, debug = 0):\n",
    "    func = switcher.get(argument) # gets just the name of the function from switcher\n",
    "    return func(runID, debug) # returns the call to the function\n",
    "  \n",
    "def runQueries(runID = 1, randomize = 0, debug = 0):\n",
    "  qList = list(range(1,numQueries+1)) # python is non inclusive of last number\n",
    "  print (\"---------------------------------\")\n",
    "  print (\"Run ID: {}\".format(runID))\n",
    "  print (\"---------------------------------\")\n",
    "  \n",
    "  if(randomize == 1):\n",
    "    seed_val = runID * 100\n",
    "    random.seed(seed_val)\n",
    "    random.shuffle(qList)\n",
    "  for queryNum in qList:\n",
    "    run_function(queryNum, runID, debug)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">---------------------------------\n",
       "Run ID: 1\n",
       "---------------------------------\n",
       "query1\n",
       "query25\n",
       "query5\n",
       "query19\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query20\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query16\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query28\n",
       "query21\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query6\n",
       "query22\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query34\n",
       "query23\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query31\n",
       "query13\n",
       "query29\n",
       "query36\n",
       "query8\n",
       "query11\n",
       "query7\n",
       "query2\n",
       "query9\n",
       "query15\n",
       "query3\n",
       "query27\n",
       "query18\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query4\n",
       "query32\n",
       "query17\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query14\n",
       "query33\n",
       "query24\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query26\n",
       "query12\n",
       "query35\n",
       "query30\n",
       "query10\n",
       "---------------------------------\n",
       "Run ID: 2\n",
       "---------------------------------\n",
       "query22\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query24\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query5\n",
       "query36\n",
       "query17\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query35\n",
       "query16\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query13\n",
       "query19\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query34\n",
       "query11\n",
       "query26\n",
       "query32\n",
       "query20\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query12\n",
       "query33\n",
       "query4\n",
       "query7\n",
       "query8\n",
       "query21\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query25\n",
       "query9\n",
       "query28\n",
       "query31\n",
       "query27\n",
       "query6\n",
       "query29\n",
       "query15\n",
       "query23\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query1\n",
       "query30\n",
       "query18\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query10\n",
       "query2\n",
       "query14\n",
       "query3\n",
       "---------------------------------\n",
       "Run ID: 3\n",
       "---------------------------------\n",
       "query16\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query21\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query7\n",
       "query13\n",
       "query31\n",
       "query24\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query3\n",
       "query18\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query2\n",
       "query34\n",
       "query20\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query9\n",
       "query10\n",
       "query22\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query15\n",
       "query33\n",
       "query8\n",
       "query5\n",
       "query26\n",
       "query12\n",
       "query11\n",
       "query28\n",
       "query4\n",
       "query25\n",
       "query6\n",
       "query30\n",
       "query19\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query14\n",
       "query29\n",
       "query27\n",
       "query17\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query35\n",
       "query32\n",
       "query1\n",
       "query36\n",
       "query23\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "---------------------------------\n",
       "Run ID: 4\n",
       "---------------------------------\n",
       "query12\n",
       "query11\n",
       "query26\n",
       "query34\n",
       "query23\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query35\n",
       "query3\n",
       "query29\n",
       "query21\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query19\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query16\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query22\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query4\n",
       "query24\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query36\n",
       "query13\n",
       "query1\n",
       "query8\n",
       "query2\n",
       "query5\n",
       "query27\n",
       "query7\n",
       "query15\n",
       "query10\n",
       "query9\n",
       "query18\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query31\n",
       "query28\n",
       "query25\n",
       "query33\n",
       "query14\n",
       "query30\n",
       "query32\n",
       "query6\n",
       "query17\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query20\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "---------------------------------\n",
       "Run ID: 5\n",
       "---------------------------------\n",
       "query23\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query12\n",
       "query29\n",
       "query21\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query27\n",
       "query9\n",
       "query22\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query33\n",
       "query14\n",
       "query16\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query15\n",
       "query5\n",
       "query18\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query20\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query6\n",
       "query32\n",
       "query2\n",
       "query10\n",
       "query13\n",
       "query8\n",
       "query28\n",
       "query31\n",
       "query1\n",
       "query3\n",
       "query24\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query19\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query35\n",
       "query11\n",
       "query4\n",
       "query26\n",
       "query7\n",
       "query25\n",
       "query17\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query36\n",
       "query34\n",
       "query30\n",
       "---------------------------------\n",
       "Run ID: 6\n",
       "---------------------------------\n",
       "query36\n",
       "query34\n",
       "query8\n",
       "query5\n",
       "query33\n",
       "query19\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query24\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query21\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query9\n",
       "query32\n",
       "query16\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query28\n",
       "query20\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query25\n",
       "query27\n",
       "query26\n",
       "query6\n",
       "query4\n",
       "query11\n",
       "query12\n",
       "query30\n",
       "query1\n",
       "query22\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query18\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query3\n",
       "query35\n",
       "query2\n",
       "query14\n",
       "query15\n",
       "query31\n",
       "query13\n",
       "query7\n",
       "query10\n",
       "query29\n",
       "query23\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "query17\n",
       "Skipping this query as it gives memory error for larger datasets\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specify numBenchmarkRuns + 2 here since we discard 1st run and python is not inclusive of last number\n",
    "for runID in range(1,numBenchmarkRuns+2):\n",
    "  runQueries(runID = runID,randomize = randomize, debug = debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/mnt/blob/timelogs/PySpark_dataset_200MB_20190327_0204.csv\n",
       "+--------+---------+-------------+---------+-----+-------------------+--------------------+----------+\n",
       "Language|Randomize|      Dataset|MachineID|RunID|               Type|           Operation| TimeTaken|\n",
       "+--------+---------+-------------+---------+-----+-------------------+--------------------+----------+\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation|              Filter|  4.652553|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Split 1 Column i...| 11.766997|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation|         Running Sum|  21.62674|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Merge 5 columns ...| 12.135535|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation| Writing 100 new ...| 10.018933|\n",
       " PySpark|        1|dataset_200MB|        2|    1|    Mixed Operation| Pivot 1 Rows and...| 7.2433767|\n",
       " PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|   GroupBy 5 columns|  37.67363|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Desc 5 c...|  60.96046|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Merge 10 columns...|  12.08664|\n",
       " PySpark|        1|dataset_200MB|        2|    1|    Mixed Operation| Pivot 10 Rows an...| 16.581488|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation| Writing 10000 ne...|   16.6762|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Asc 10 c...|  49.60384|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation| Writing 1000 new...| 10.504343|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation|     Filter Reg Ex 1| 2.6379268|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Asc 1 co...| 48.144398|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Mathematical Ope...| 1.5376029|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation|     Filter Reg Ex 2| 2.8180661|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Merge 2 columns ...| 13.313459|\n",
       " PySpark|        1|dataset_200MB|        2|    1|      Row Operation|         Shift (Lag)| 15.894673|\n",
       " PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|  GroupBy 10 columns| 51.195763|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Desc 10 ...| 32.560555|\n",
       " PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|    Ranking by Group| 13.500582|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Split 1 Column i...| 13.547101|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Desc 1 c...| 42.490295|\n",
       " PySpark|        1|dataset_200MB|        2|    1|    Mixed Operation| Pivot 5 Rows and...| 12.300534|\n",
       " PySpark|        1|dataset_200MB|        2|    1|Aggregate Operation|    GroupBy 1 column|  18.97193|\n",
       " PySpark|        1|dataset_200MB|        2|    1|   Column Operation| Sorting Asc 5 co...|  51.04864|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation|         Running Sum| 12.951963|\n",
       " PySpark|        1|dataset_200MB|        2|    2|    Mixed Operation| Pivot 10 Rows an...| 14.890925|\n",
       " PySpark|        1|dataset_200MB|        2|    2|    Mixed Operation| Pivot 5 Rows and...| 13.132951|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Desc 5 c...|   50.8436|\n",
       " PySpark|        1|dataset_200MB|        2|    2|    Mixed Operation| Pivot 1 Rows and...|  4.913946|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Asc 10 c...|    66.548|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Split 1 Column i...| 12.993703|\n",
       " PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|  GroupBy 10 columns|  48.24642|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Desc 1 c...| 63.871933|\n",
       " PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|    Ranking by Group| 12.241798|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation|         Shift (Lag)| 15.150602|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation| Writing 1000 new...|  9.472681|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation| Writing 10000 ne...| 12.540118|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Split 1 Column i...|  13.99193|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Asc 1 co...| 36.726906|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Merge 5 columns ...| 13.188966|\n",
       " PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|   GroupBy 5 columns| 32.697876|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Merge 2 columns ...| 10.108783|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation| Writing 100 new ...|  9.384319|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Merge 10 columns...|  15.41621|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Mathematical Ope...|  1.790127|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation|              Filter|     3.818|\n",
       " PySpark|        1|dataset_200MB|        2|    2|Aggregate Operation|    GroupBy 1 column| 17.441383|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Asc 5 co...| 42.837223|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation|     Filter Reg Ex 1| 2.4659255|\n",
       " PySpark|        1|dataset_200MB|        2|    2|   Column Operation| Sorting Desc 10 ...| 44.827015|\n",
       " PySpark|        1|dataset_200MB|        2|    2|      Row Operation|     Filter Reg Ex 2| 2.5977216|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation| Writing 1000 new...| 11.373891|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Desc 5 c...|  48.61929|\n",
       " PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|   GroupBy 5 columns| 30.702564|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation|     Filter Reg Ex 2| 2.5244195|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation|     Filter Reg Ex 1| 2.9178982|\n",
       " PySpark|        1|dataset_200MB|        2|    3|    Mixed Operation| Pivot 1 Rows and...| 4.6772213|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Asc 1 co...| 40.202076|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Asc 5 co...|  68.24492|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Mathematical Ope...| 1.4702477|\n",
       " PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|    Ranking by Group| 12.814879|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation| Writing 10000 ne...| 11.759948|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation|         Running Sum| 16.353033|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Split 1 Column i...|  15.00292|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Desc 1 c...|  38.45776|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Asc 10 c...|  41.22801|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Merge 5 columns ...| 14.163449|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation|         Shift (Lag)|  20.64348|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Split 1 Column i...| 12.753967|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation| Writing 100 new ...|10.1632595|\n",
       " PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|    GroupBy 1 column| 18.233896|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Sorting Desc 10 ...|   60.7628|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Merge 10 columns...| 14.033956|\n",
       " PySpark|        1|dataset_200MB|        2|    3|   Column Operation| Merge 2 columns ...|14.6378565|\n",
       " PySpark|        1|dataset_200MB|        2|    3|    Mixed Operation| Pivot 5 Rows and...|12.7107525|\n",
       " PySpark|        1|dataset_200MB|        2|    3|Aggregate Operation|  GroupBy 10 columns| 48.964245|\n",
       " PySpark|        1|dataset_200MB|        2|    3|      Row Operation|              Filter| 3.6591258|\n",
       " PySpark|        1|dataset_200MB|        2|    3|    Mixed Operation| Pivot 10 Rows an...| 13.787422|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Desc 1 c...|  66.44919|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Asc 10 c...| 50.456165|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Split 1 Column i...| 16.947489|\n",
       " PySpark|        1|dataset_200MB|        2|    4|    Mixed Operation| Pivot 1 Rows and...|  5.032236|\n",
       " PySpark|        1|dataset_200MB|        2|    4|    Mixed Operation| Pivot 5 Rows and...| 11.582377|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation|     Filter Reg Ex 2| 2.6108155|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Merge 10 columns...| 13.094477|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation|         Shift (Lag)| 17.706583|\n",
       " PySpark|        1|dataset_200MB|        2|    4|    Mixed Operation| Pivot 10 Rows an...| 14.068739|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Desc 5 c...|  39.48249|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation|              Filter|  6.260732|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation| Writing 10000 ne...| 17.976023|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation|     Filter Reg Ex 1| 3.2467313|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation|         Running Sum| 17.831646|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Merge 2 columns ...|13.8633795|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation| Writing 1000 new...|  8.709115|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Mathematical Ope...| 1.5214925|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Asc 5 co...| 25.357311|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Asc 1 co...| 24.158466|\n",
       " PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|   GroupBy 5 columns|  35.22343|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Merge 5 columns ...| 12.937048|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Split 1 Column i...|  21.15601|\n",
       " PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|    Ranking by Group|  28.56262|\n",
       " PySpark|        1|dataset_200MB|        2|    4|   Column Operation| Sorting Desc 10 ...| 50.058784|\n",
       " PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|    GroupBy 1 column| 21.157274|\n",
       " PySpark|        1|dataset_200MB|        2|    4|Aggregate Operation|  GroupBy 10 columns|  46.28201|\n",
       " PySpark|        1|dataset_200MB|        2|    4|      Row Operation| Writing 100 new ...|  10.12348|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Desc 1 c...| 56.924442|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Merge 10 columns...| 17.753628|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Merge 2 columns ...| 13.721421|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Asc 1 co...|  45.83067|\n",
       " PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|    Ranking by Group| 12.706296|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Desc 10 ...|  38.33555|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Mathematical Ope...| 1.4980462|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation|         Running Sum| 17.857409|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation| Writing 100 new ...| 11.425949|\n",
       " PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|  GroupBy 10 columns|   50.6067|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation|     Filter Reg Ex 1|  2.546941|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Asc 5 co...|  58.41099|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Desc 5 c...|  93.48637|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation| Writing 10000 ne...| 14.819243|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Merge 5 columns ...|  16.85726|\n",
       " PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|   GroupBy 5 columns|  69.88675|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation|              Filter|  7.709294|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation|     Filter Reg Ex 2| 5.0929585|\n",
       " PySpark|        1|dataset_200MB|        2|    5|    Mixed Operation| Pivot 5 Rows and...| 15.124113|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Sorting Asc 10 c...|  52.40623|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation|         Shift (Lag)| 15.689404|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Split 1 Column i...| 13.004327|\n",
       " PySpark|        1|dataset_200MB|        2|    5|      Row Operation| Writing 1000 new...|12.6085415|\n",
       " PySpark|        1|dataset_200MB|        2|    5|   Column Operation| Split 1 Column i...| 13.583167|\n",
       " PySpark|        1|dataset_200MB|        2|    5|    Mixed Operation| Pivot 10 Rows an...| 14.068534|\n",
       " PySpark|        1|dataset_200MB|        2|    5|    Mixed Operation| Pivot 1 Rows and...| 4.6140976|\n",
       " PySpark|        1|dataset_200MB|        2|    5|Aggregate Operation|    GroupBy 1 column| 18.561369|\n",
       " PySpark|        1|dataset_200MB|        2|    6|    Mixed Operation| Pivot 10 Rows an...| 13.586413|\n",
       " PySpark|        1|dataset_200MB|        2|    6|    Mixed Operation| Pivot 1 Rows and...|  4.985581|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation| Writing 10000 ne...| 14.288491|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation|         Running Sum|   15.5333|\n",
       " PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|    Ranking by Group|  16.38609|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Asc 1 co...| 39.986095|\n",
       " PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|  GroupBy 10 columns| 71.580795|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Merge 5 columns ...|  11.18101|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Split 1 Column i...| 14.317528|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Merge 2 columns ...| 12.640671|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Split 1 Column i...| 12.883596|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation| Writing 100 new ...|  9.747298|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation|         Shift (Lag)| 20.916378|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Asc 10 c...|  58.30399|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Desc 1 c...| 66.805405|\n",
       " PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|    GroupBy 1 column|  17.41619|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation|              Filter| 3.4869204|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation|     Filter Reg Ex 2| 2.3455906|\n",
       " PySpark|        1|dataset_200MB|        2|    6|    Mixed Operation| Pivot 5 Rows and...| 12.038018|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation|     Filter Reg Ex 1| 2.9779055|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Desc 10 ...|  73.92443|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Mathematical Ope...| 1.4425983|\n",
       " PySpark|        1|dataset_200MB|        2|    6|Aggregate Operation|   GroupBy 5 columns| 32.986137|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Desc 5 c...|  73.54623|\n",
       " PySpark|        1|dataset_200MB|        2|    6|      Row Operation| Writing 1000 new...| 12.545465|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Sorting Asc 5 co...| 83.954605|\n",
       " PySpark|        1|dataset_200MB|        2|    6|   Column Operation| Merge 10 columns...|  16.22411|\n",
       "+--------+---------+-------------+---------+-----+-------------------+--------------------+----------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_schema = [StructField('Language',StringType(),True) \n",
    "              ,StructField('Randomize',IntegerType(),True) \n",
    "              ,StructField('Dataset',StringType(),True)\n",
    "              ,StructField('MachineID',IntegerType(),True) \n",
    "              ,StructField('RunID',IntegerType(),True)\n",
    "              ,StructField('Type',StringType(),True) \n",
    "              ,StructField('Operation',StringType(),True)\n",
    "              ,StructField('TimeTaken',FloatType(),True)]\n",
    "\n",
    "final_struct = StructType(fields = data_schema)\n",
    "\n",
    "print(logfilename)\n",
    "timelog = spark.read.csv(logfilename, schema = final_struct)\n",
    "logging.getLogger().setLevel(logging.WARNING) # supress all informational items\n",
    "timelog.show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/datasets/</td><td>datasets/</td><td>0</td></tr><tr><td>dbfs:/mnt/blob/timelogs/</td><td>timelogs/</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displays all files in a location\n",
    "# https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\n",
    "display(dbutils.fs.ls(\"dbfs:/mnt/blob/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB.csv</td><td>dataset_100MB.csv</td><td>121258369</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_100.csv</td><td>dataset_100MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_1000.csv</td><td>dataset_100MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_10000.csv</td><td>dataset_100MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB.csv</td><td>dataset_10MB.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_100.csv</td><td>dataset_10MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_1000.csv</td><td>dataset_10MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_10000.csv</td><td>dataset_10MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB.csv</td><td>dataset_1GB.csv</td><td>1204049059</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB_add_100.csv</td><td>dataset_1GB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB_add_1000.csv</td><td>dataset_1GB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_1GB_add_10000.csv</td><td>dataset_1GB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB.csv</td><td>dataset_200MB.csv</td><td>241022871</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_100.csv</td><td>dataset_200MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_1000.csv</td><td>dataset_200MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_10000.csv</td><td>dataset_200MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB.csv</td><td>dataset_300MB.csv</td><td>359945718</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_100.csv</td><td>dataset_300MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_1000.csv</td><td>dataset_300MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_10000.csv</td><td>dataset_300MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB.csv</td><td>dataset_500MB.csv</td><td>604690884</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_100.csv</td><td>dataset_500MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_1000.csv</td><td>dataset_500MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_10000.csv</td><td>dataset_500MB_add_10000.csv</td><td>12036588</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/mnt/blob/datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_100MB_20190323_1935.csv</td><td>PySpark_dataset_100MB_20190323_1935.csv</td><td>15886</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_10MB_20190323_1900.csv</td><td>PySpark_dataset_10MB_20190323_1900.csv</td><td>17117</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_200MB_20190327_0204.csv</td><td>PySpark_dataset_200MB_20190327_0204.csv</td><td>12940</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_300MB_20190326_2328.csv</td><td>PySpark_dataset_300MB_20190326_2328.csv</td><td>12945</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_500MB_20190323_2332.csv</td><td>PySpark_dataset_500MB_20190323_2332.csv</td><td>12976</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_100MB_20190383_0610.csv</td><td>time_Scala_Random_dataset_100MB_20190383_0610.csv</td><td>14332</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_10MB_20190383_0554.csv</td><td>time_Scala_Random_dataset_10MB_20190383_0554.csv</td><td>15421</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190385_1025.csv</td><td>time_Scala_Random_dataset_200MB_20190385_1025.csv</td><td>11629</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_300MB_20190385_0916.csv</td><td>time_Scala_Random_dataset_300MB_20190385_0916.csv</td><td>11680</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_0406.csv</td><td>time_Scala_Random_dataset_500MB_20190386_0406.csv</td><td>7794</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_1205.csv</td><td>time_Scala_Random_dataset_500MB_20190386_1205.csv</td><td>5842</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/mnt/blob/timelogs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>True\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DELETING FILES (USE WITH CARE)\n",
    "# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_0314.csv\")\n",
    "# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190386_0557.csv\")\n",
    "# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190384_1209.csv\")\n",
    "# dbutils.fs.rm(\"/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190384_0427.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Moving a File\n",
    "# https://forums.databricks.com/questions/14312/how-to-move-files-of-same-extension-in-databricks.html\n",
    "# dbutils.fs.mv(\"dbfs:/mnt/blob/add_100.csv\", \"dbfs:/mnt/blob/datasets/.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "Project_PySpark_v2",
  "notebookId": 4088465828014280
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
