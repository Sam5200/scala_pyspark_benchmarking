{"cells":[{"cell_type":"code","source":["# For PySpark\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql.types import (StructField, StructType, StringType, IntegerType, FloatType, DateType)\nfrom pyspark.sql.functions import lag, col, concat, lit\nfrom pyspark.sql.functions import *\n\n# For Time Logging\nfrom contextlib import contextmanager\nimport logging\nimport time\nimport datetime\n\n# For error checks\nimport sys\n\n# For randomizing queries\nimport random\n\n# Print Function name\nimport inspect"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Setup\nfile_loc = \"/dbfs\"\nfile_prefix = \"/mnt/blob/datasets/\"\nfile_name = \"dataset_10MB\"\n\ndebug = 0\nrandomize = 1\nnumBenchmarkRuns = 5 # 5 is the default\n\n# 1 for Max\n# 2 for Nikhil\nmachineID = 1\n\n# Total Number of Queries = 36\nnumQueries = 36"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Auto calculations (DO NOT CHANGE)\n# Make the directories if they do not exist\ndbutils.fs.mkdirs(\"mnt/blob/timelogs\")\n# dbutils.fs.mkdirs(\"mnt/blob/datasets\") # Should exist since we have already created the dataset\n\nnow = datetime.datetime.now()\nstart_of_run  = now.strftime(\"%Y%m%d_%H%M\")\nlogfilename = \"/mnt/blob/timelogs/PySpark_\" + file_name + \"_\" + start_of_run + \".csv\"\nlogfilename_withDBFS = \"/dbfs\" + logfilename"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["@contextmanager\ndef time_usage(runID, name=\"\"):\n    \"\"\"log the time usage in a code block\n    prefix: the prefix text to show\n    \"\"\"\n    #print (\"In time_usage runID = {}\".format(runID))\n    start = time.time()\n    yield\n    end = time.time()\n    elapsed_seconds = float(\"%.10f\" % (end - start))\n    logging.info('%s: elapsed seconds: %s', name, elapsed_seconds)\n    output = \"\\n\" + str(runID) + \",\" + name + \",\" + str(elapsed_seconds)\n       \n    # https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\n    # https://stackoverflow.com/questions/49318402/read-write-single-file-in-databricks\n    with open(logfilename_withDBFS,\"a\") as file:\n      file.write(\"\\n\" + \"PySpark\" + \",\" \n                 + str(randomize) + \",\" \n                 + file_name + \",\" \n                 + str(machineID) + \",\" \n                 + str(runID) + \",\" \n                 + name + \",\" \n                 + str(elapsed_seconds))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#logging.getLogger().setLevel(logging.INFO)\nlogging.getLogger().setLevel(logging.WARNING)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["print (\"Loading Data\")\ndata = spark.read.csv(file_prefix + file_name + \".csv\", inferSchema = True, header=True)\ndata_join = data.limit(int(data.count()/2))\ndata_100 = spark.read.csv(file_prefix + file_name + \"_add_100.csv\", inferSchema = True, header=True)\ndata_1000 = spark.read.csv(file_prefix + file_name + \"_add_1000.csv\", inferSchema = True, header=True)\ndata_10000 = spark.read.csv(file_prefix + file_name + \"_add_10000.csv\", inferSchema = True, header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading Data\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# testing\ndata_join.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>5000\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["if data_100.count() != 100:\n  sys.exit(\"Num Rows Expected was 100. Got something else.\")\nif data_1000.count() != 1000:\n  sys.exit(\"Num Rows Expected was 1000. Got something else.\")\nif data_10000.count() != 10000:\n  sys.exit(\"Num Rows Expected was 10000. Got something else.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["data.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n[&apos;float0&apos;,\n &apos;float1&apos;,\n &apos;float2&apos;,\n &apos;float3&apos;,\n &apos;float4&apos;,\n &apos;float5&apos;,\n &apos;float6&apos;,\n &apos;float7&apos;,\n &apos;float8&apos;,\n &apos;float9&apos;,\n &apos;float10&apos;,\n &apos;float11&apos;,\n &apos;float12&apos;,\n &apos;float13&apos;,\n &apos;float14&apos;,\n &apos;float15&apos;,\n &apos;float16&apos;,\n &apos;float17&apos;,\n &apos;float18&apos;,\n &apos;float19&apos;,\n &apos;int0&apos;,\n &apos;int1&apos;,\n &apos;int2&apos;,\n &apos;int3&apos;,\n &apos;int4&apos;,\n &apos;int5&apos;,\n &apos;int6&apos;,\n &apos;int7&apos;,\n &apos;int8&apos;,\n &apos;int9&apos;,\n &apos;int10&apos;,\n &apos;int11&apos;,\n &apos;int12&apos;,\n &apos;int13&apos;,\n &apos;int14&apos;,\n &apos;int15&apos;,\n &apos;int16&apos;,\n &apos;int17&apos;,\n &apos;int18&apos;,\n &apos;int19&apos;,\n &apos;words0&apos;,\n &apos;words1&apos;,\n &apos;words2&apos;,\n &apos;words3&apos;,\n &apos;words4&apos;,\n &apos;words5&apos;,\n &apos;words6&apos;,\n &apos;words7&apos;,\n &apos;words8&apos;,\n &apos;words9&apos;,\n &apos;group0&apos;,\n &apos;group1&apos;,\n &apos;group2&apos;,\n &apos;group3&apos;,\n &apos;group4&apos;,\n &apos;group5&apos;,\n &apos;group6&apos;,\n &apos;group7&apos;,\n &apos;group8&apos;,\n &apos;group9&apos;,\n &apos;group10&apos;]\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# https://jaxenter.com/implement-switch-case-statement-python-138315.html\n\ndef query1(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # filtering rows based on a condition\n  with time_usage(runID,\"Row Operation, Filter\"):\n    temp = data.filter((data['int17'] > 200) & (data['float17'] < 200)).collect()\n  if debug >= 1:\n    print(temp[0])\n\ndef query2(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # Filtering rows based on regular expressions\n  # expr = \"^troubleshot\"\n  expr = \"^overmeddled\"\n  with time_usage(runID,\"Row Operation, Filter Reg Ex 1\"):\n    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()  \n\ndef query3(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # lf at 3rd and 4th position and ending with w\n  expr = \".{2}lf.*s$\" \n  with time_usage(runID,\"Row Operation, Filter Reg Ex 2\"):\n    temp = data.filter(data[\"Group0\"].rlike(expr)).collect()\n  if debug >= 1:\n    print(temp[0]['group0']) # check\n\ndef query4(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # Shift (Lag) operation\n  w = Window.orderBy(\"Int1\")\n  with time_usage(runID,\"Row Operation, Shift (Lag)\"):\n    temp = data.withColumn('status_lag', lag(col('Int1')).over(w)).collect()\n  if debug >= 1:\n    print(temp[1]['status_lag'])\n  \ndef query5(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # running sum\n  # https://stackoverflow.com/questions/46982119/how-to-calculate-cumulative-sum-in-a-pyspark-table\n  window = Window.orderBy(\"Int1\")\n  with time_usage(runID,\"Row Operation, Running Sum\"):\n    temp = data.withColumn(\"CumSumTotal\", sum(data['Int1']).over(window)).collect()\n  if debug >= 1:\n    print(temp[1]['CumSumTotal'])\n  \ndef query6(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Row Operation, Writing 100 new rows\"):\n    temp = data.union(data_100).collect()\n  \ndef query7(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Row Operation, Writing 1000 new rows\"):\n    data.union(data_1000).collect()  \n\ndef query8(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Row Operation, Writing 10000 new rows\"):\n    data.union(data_10000).collect()\n    \ndef query9(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Asc 1 column\"):\n    temp = data.orderBy(\"words0\").collect()\n  temp[0]['words0']\n  \ndef query10(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Asc 5 column\"):\n    temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\").collect()\n  if debug >= 1:\n    print(temp[0]['words1'])\n  \ndef query11(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Asc 10 column\"):\n      temp = data.orderBy(\"words0\",\"words1\",\"words2\",\"words3\",\"words4\",\"words5\",\"words6\",\"words7\",\"words8\",\"words9\").collect()\n  if debug >= 1:\n    print(temp[0]['words2'])\n  \ndef query12(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Desc 1 column\"):\n    temp = data.orderBy(data[\"words0\"].desc()).collect()\n  if debug >= 1:\n    print(temp[0]['words0'])\n  \ndef query13(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Desc 5 column\"):\n    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc()).collect()\n  temp[0]['words1']\n  \ndef query14(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Sorting Desc 10 column\"):\n    temp = data.orderBy(data[\"words0\"].desc(),data[\"words1\"].desc(),data[\"words2\"].desc(),data[\"words3\"].desc(),data[\"words4\"].desc(),data[\"words5\"].desc(),data[\"words6\"].desc(),data[\"words7\"].desc(),data[\"words8\"].desc(),data[\"words9\"].desc()).collect()\n  temp[0]['words2'] \n  \ndef query15(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # aggregate function will apply function to the whole dataframe (not grouping by a column)\n  # takes a dictionary as input\n  # however this may not be the best way to do this: https://stackoverflow.com/a/51855775\n  with time_usage(runID,\"Column Operation, Mathematical Operation on Columns\"):\n    temp = data.select(['Int1','Float1','Int2','Float3','Float10']).agg({'Int1':'sum','Float1':'avg','Int2':'count','Float3':'min','Float10':'max'}).collect()\n  temp[0]\n\ndef query16(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n   \n  # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n  if (file_name == \"dataset_10MB\"):\n    with time_usage(runID,\"Column Operation, Inner Join 3 Columns\"):\n      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'inner').collect()\n    if debug >= 1:\n      print(temp[0]['group0'])\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query17(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Inner Join 5 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'inner').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n\ndef query18(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Inner Join 10 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'inner').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query19(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n    \n  if (file_name == \"dataset_10MB\"):\n    with time_usage(runID,\"Column Operation, Left Outer Join 3 Columns\"):\n      temp = data.join(data_join, ['group0', 'group1', 'group2'], how = 'left_outer').collect()\n    if debug >= 1:\n      print(temp[0]['group1'])\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query20(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Left Outer Join 5 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'left_outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query21(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Left Outer Join 10 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'left_outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query22(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if (file_name == \"dataset_10MB\"):\n    with time_usage(runID,\"Column Operation, Full Outer Join 3 Columns\"):\n      temp = data.join(data_join, ['group0','group1','group2'], how = 'outer').collect()\n    if debug >= 1:\n      print(temp[0]['group2'])\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n\ndef query23(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  \n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Full Outer Join 5 Columns\"):\n      data.join(data_join, ['group0','group1','group2','group3','group4'], how = 'outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n    \ndef query24(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  if ((file_name == \"dataset_10MB\") | (file_name == \"dataset_100MB\")):\n    with time_usage(runID,\"Column Operation, Full Outer Join 10 Columns\"):\n      data.join(data_join, ['group0','group1','group2','group3','group4','group5','group6','group7','group8','group9'], how = 'outer').collect()\n  else:\n    print (\"Skipping this query as it gives memory error for larger datasets\")\n\ndef query25(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n  with time_usage(runID,\"Column Operation, Split 1 Column into 5\"):\n    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n    split_col.getItem(1)\n    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n    data_new = data_new.collect()\n  if debug >= 1:\n    print (data_new[0]['words0_0'])\n    print (data_new[0]['words0_1'])\n    print (data_new[0]['words0_2'])\n\ndef query26(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Split 1 Column into 10\"):\n    split_col = split(data[\"words0\"], '\\\\ |a|e|i|o|u')\n    data_new = data.withColumn(\"words0_0\", split_col.getItem(0))\n    data_new = data_new.withColumn(\"words0_1\", split_col.getItem(1))\n    data_new = data_new.withColumn(\"words0_2\", split_col.getItem(2))\n    data_new = data_new.withColumn(\"words0_3\", split_col.getItem(3))\n    data_new = data_new.withColumn(\"words0_4\", split_col.getItem(4))\n    data_new = data_new.withColumn(\"words0_5\", split_col.getItem(5))\n    data_new = data_new.withColumn(\"words0_6\", split_col.getItem(6))\n    data_new = data_new.withColumn(\"words0_7\", split_col.getItem(7))\n    data_new = data_new.withColumn(\"words0_8\", split_col.getItem(8))\n    data_new = data_new.withColumn(\"words0_9\", split_col.getItem(9))\n    data_new = data_new.collect()\n  if debug >= 1:\n    print (data_new[0]['words0'])\n    print (data_new[0]['words0_1'])\n    print (data_new[0]['words0_2'])\n    print (data_new[0]['words0_3'])\n    print (data_new[0]['words0_7'])\n    print (data_new[0]['words0_8'])\n    print (data_new[0]['words0_9'])\n  \ndef query27(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # https://www.edureka.co/community/2280/concatenate-columns-in-apache-spark-dataframe\n  with time_usage(runID,\"Column Operation, Merge 2 columns into 1\"):\n    temp = data.withColumn(\"words0m1\", concat(col(\"words0\")  , lit(\" \"), col(\"words1\") )).collect()\n  if debug >= 1:\n    print(temp[0]['words0m1'])\n\ndef query28(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Merge 5 columns into 1\"):\n    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n                                       ,col(\"words1\"), lit(\" \")\n                                       ,col(\"words2\"), lit(\" \")\n                                       ,col(\"words3\"), lit(\" \")\n                                       ,col(\"words4\")\n                                      )\n                   ).collect()\n\ndef query29(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Column Operation, Merge 10 columns into 1\"):\n    temp = data.withColumn(\"words0m1\", concat(col(\"words0\") , lit(\" \")\n                                       ,col(\"words1\"), lit(\" \")\n                                       ,col(\"words2\"), lit(\" \")\n                                       ,col(\"words3\"), lit(\" \")\n                                       ,col(\"words4\"), lit(\" \")\n                                       ,col(\"words5\"), lit(\" \")\n                                       ,col(\"words6\"), lit(\" \")\n                                       ,col(\"words7\"), lit(\" \")\n                                       ,col(\"words8\"), lit(\" \")\n                                       ,col(\"words9\")\n                                      )\n                   ).collect()\n  \ndef query30(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy (1 group)\n  with time_usage(runID,\"Aggregate Operation, GroupBy 1 column\"):\n    temp1 = data.groupBy(\"Group0\").count().collect()\n    temp2 = data.groupBy(\"Group0\").sum().collect()\n    temp3 = data.groupBy(\"Group0\").avg().collect()\n    temp4 = data.groupBy(\"Group0\").min().collect()\n    temp5 = data.groupBy(\"Group0\").max().collect()\n  if debug >= 1:\n    print(temp1[0])\n    print(temp2[0])\n    print(temp3[0])\n    print(temp4[0])\n    print(temp5[0])\n\n\ndef query31(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy (5 groups)\n  with time_usage(runID,\"Aggregate Operation, GroupBy 5 columns\"):\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').count().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').sum().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').avg().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').min().collect()\n    data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8').max().collect()\n\ndef query32(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n   #GroupBy (10 groups)\n  with time_usage(runID,\"Aggregate Operation, GroupBy 10 columns\"):\n     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').count().collect()\n     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').sum().collect()\n     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').avg().collect()\n     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').min().collect()\n     data.groupBy(\"Group0\",'Group2','Group4','Group6','Group8','Group1','Group3','Group5','Group7','Group9').max().collect()\n\n    \ndef query33(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # GroupBy with ranking\n  # https://stackoverflow.com/a/41662162\n  with time_usage(runID,\"Aggregate Operation, Ranking by Group\"):\n    temp = data.withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"Group0\").orderBy(desc(\"Int1\")))).collect()\n  if debug >= 1:\n    print(temp[0][\"rank\"])\n    print(temp[0][\"group0\"])\n    print(temp[0][\"int1\"])  \n  \ndef query34(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  # Scala Syntax is identical to PySpark\n  # df.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").count()\n  with time_usage(runID,\"Mixed Operation, Pivot 1 Rows and 1 Column\"):\n    temp = data.groupBy(\"group0\").pivot(\"group10\").sum(\"float0\").collect()\n  if debug >= 1:\n    print(temp[0])\n    \ndef query35(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Mixed Operation, Pivot 5 Rows and 1 Column\"):\n    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\").pivot(\"group10\").sum(\"float1\").collect()\n  if debug >= 1:\n    print(temp[0])\n  \ndef query36(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  with time_usage(runID,\"Mixed Operation, Pivot 10 Rows and 1 Column\"):\n    temp = data.groupBy(\"group0\",\"group1\",\"group2\",\"group3\",\"group4\",\"group5\",\"group6\",\"group7\",\"group8\",\"group9\").pivot(\"group10\").sum(\"float2\").collect()\n  if debug >= 1:\n    print(temp[0])\n\ndef unexpected(runID, debug = 0):\n  print(inspect.stack()[0][3]) # Prints function name\n  print(\"Unexpected case\")\n\nswitcher = {\n  1: query1,\n  2: query2,\n  3: query3,\n  4: query4,\n  5: query5,\n  6: query6,\n  7: query7,\n  8: query8,\n  9: query9,\n  10: query10,\n  11: query11,\n  12: query12,\n  13: query13,\n  14: query14,\n  15: query15,\n  16: query16,\n  17: query17,\n  18: query18,\n  19: query19,\n  20: query20,\n  21: query21,\n  22: query22,\n  23: query23,\n  24: query24,\n  25: query25,\n  26: query26,\n  27: query27,\n  28: query28,\n  29: query29,\n  30: query30,\n  31: query31,\n  32: query32,\n  33: query33,\n  34: query34,\n  35: query35,\n  36: query36,\n  \"whoa\": unexpected\n}\n \ndef run_function(argument, runID, debug = 0):\n    func = switcher.get(argument) # gets just the name of the function from switcher\n    return func(runID, debug) # returns the call to the function\n  \ndef runQueries(runID = 1, randomize = 0, debug = 0):\n  qList = list(range(1,numQueries+1)) # python is non inclusive of last number\n  print (\"---------------------------------\")\n  print (\"Run ID: {}\".format(runID))\n  print (\"---------------------------------\")\n  \n  if(randomize == 1):\n    seed_val = runID * 100\n    random.seed(seed_val)\n    random.shuffle(qList)\n  for queryNum in qList:\n    run_function(queryNum, runID, debug)   \n    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# specify numBenchmarkRuns + 2 here since we discard 1st run and python is not inclusive of last number\nfor runID in range(1,numBenchmarkRuns+2):\n  runQueries(runID = runID,randomize = randomize, debug = debug)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">---------------------------------\nRun ID: 1\n---------------------------------\nquery1\nquery25\nquery5\nquery19\nquery20\nquery16\nquery28\nquery21\nquery6\nquery22\nquery34\nquery23\nquery31\nquery13\nquery29\nquery36\nquery8\nquery11\nquery7\nquery2\nquery9\nquery15\nquery3\nquery27\nquery18\nquery4\nquery32\nquery17\nquery14\nquery33\nquery24\nquery26\nquery12\nquery35\nquery30\nquery10\n---------------------------------\nRun ID: 2\n---------------------------------\nquery22\nquery24\nquery5\nquery36\nquery17\nquery35\nquery16\nquery13\nquery19\nquery34\nquery11\nquery26\nquery32\nquery20\nquery12\nquery33\nquery4\nquery7\nquery8\nquery21\nquery25\nquery9\nquery28\nquery31\nquery27\nquery6\nquery29\nquery15\nquery23\nquery1\nquery30\nquery18\nquery10\nquery2\nquery14\nquery3\n---------------------------------\nRun ID: 3\n---------------------------------\nquery16\nquery21\nquery7\nquery13\nquery31\nquery24\nquery3\nquery18\nquery2\nquery34\nquery20\nquery9\nquery10\nquery22\nquery15\nquery33\nquery8\nquery5\nquery26\nquery12\nquery11\nquery28\nquery4\nquery25\nquery6\nquery30\nquery19\nquery14\nquery29\nquery27\nquery17\nquery35\nquery32\nquery1\nquery36\nquery23\n---------------------------------\nRun ID: 4\n---------------------------------\nquery12\nquery11\nquery26\nquery34\nquery23\nquery35\nquery3\nquery29\nquery21\nquery19\nquery16\nquery22\nquery4\nquery24\nquery36\nquery13\nquery1\nquery8\nquery2\nquery5\nquery27\nquery7\nquery15\nquery10\nquery9\nquery18\nquery31\nquery28\nquery25\nquery33\nquery14\nquery30\nquery32\nquery6\nquery17\nquery20\n---------------------------------\nRun ID: 5\n---------------------------------\nquery23\nquery12\nquery29\nquery21\nquery27\nquery9\nquery22\nquery33\nquery14\nquery16\nquery15\nquery5\nquery18\nquery20\nquery6\nquery32\nquery2\nquery10\nquery13\nquery8\nquery28\nquery31\nquery1\nquery3\nquery24\nquery19\nquery35\nquery11\nquery4\nquery26\nquery7\nquery25\nquery17\nquery36\nquery34\nquery30\n---------------------------------\nRun ID: 6\n---------------------------------\nquery36\nquery34\nquery8\nquery5\nquery33\nquery19\nquery24\nquery21\nquery9\nquery32\nquery16\nquery28\nquery20\nquery25\nquery27\nquery26\nquery6\nquery4\nquery11\nquery12\nquery30\nquery1\nquery22\nquery18\nquery3\nquery35\nquery2\nquery14\nquery15\nquery31\nquery13\nquery7\nquery10\nquery29\nquery23\nquery17\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["data_schema = [StructField('Language',StringType(),True) \n              ,StructField('Randomize',IntegerType(),True) \n              ,StructField('Dataset',StringType(),True)\n              ,StructField('MachineID',IntegerType(),True) \n              ,StructField('RunID',IntegerType(),True)\n              ,StructField('Type',StringType(),True) \n              ,StructField('Operation',StringType(),True)\n              ,StructField('TimeTaken',FloatType(),True)]\n\nfinal_struct = StructType(fields = data_schema)\n\nprint(logfilename)\ntimelog = spark.read.csv(logfilename, schema = final_struct)\nlogging.getLogger().setLevel(logging.WARNING) # supress all informational items\ntimelog.show(10000)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/blob/timelogs/PySpark_dataset_10MB_20190407_0546.csv\n+--------+---------+------------+---------+-----+-------------------+--------------------+----------+\nLanguage|Randomize|     Dataset|MachineID|RunID|               Type|           Operation| TimeTaken|\n+--------+---------+------------+---------+-----+-------------------+--------------------+----------+\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation|              Filter|0.32641315|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Split 1 Column i...|0.81458473|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation|         Running Sum|0.79778934|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Left Outer Join ...| 3.4056568|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Left Outer Join ...| 1.2429781|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Inner Join 3 Col...| 3.1812835|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Merge 5 columns ...|0.67389417|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Left Outer Join ...| 1.2805572|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation| Writing 100 new ...|0.53446674|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Full Outer Join ...| 3.8735538|\n PySpark|        1|dataset_10MB|        1|    1|    Mixed Operation| Pivot 1 Rows and...| 1.1556492|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Full Outer Join ...| 2.1686563|\n PySpark|        1|dataset_10MB|        1|    1|Aggregate Operation|   GroupBy 5 columns|  4.213376|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Sorting Desc 5 c...|0.96516347|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Merge 10 columns...|0.66534567|\n PySpark|        1|dataset_10MB|        1|    1|    Mixed Operation| Pivot 10 Rows an...|  2.208849|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation| Writing 10000 ne...| 0.8356645|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Sorting Asc 10 c...| 1.0931809|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation| Writing 1000 new...|0.53265023|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation|     Filter Reg Ex 1|0.35321856|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Sorting Asc 1 co...| 0.9651816|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Mathematical Ope...|0.30237222|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation|     Filter Reg Ex 2| 0.2714119|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Merge 2 columns ...| 0.5863848|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Inner Join 10 Co...| 1.2471995|\n PySpark|        1|dataset_10MB|        1|    1|      Row Operation|         Shift (Lag)|0.83017874|\n PySpark|        1|dataset_10MB|        1|    1|Aggregate Operation|  GroupBy 10 columns| 5.1243753|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Inner Join 5 Col...| 1.1914065|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Sorting Desc 10 ...|0.99989104|\n PySpark|        1|dataset_10MB|        1|    1|Aggregate Operation|    Ranking by Group| 0.9509232|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Full Outer Join ...| 2.2963343|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Split 1 Column i...|0.75069714|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Sorting Desc 1 c...|0.98221135|\n PySpark|        1|dataset_10MB|        1|    1|    Mixed Operation| Pivot 5 Rows and...| 2.0150564|\n PySpark|        1|dataset_10MB|        1|    1|Aggregate Operation|    GroupBy 1 column| 3.4933653|\n PySpark|        1|dataset_10MB|        1|    1|   Column Operation| Sorting Asc 5 co...|0.98578143|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Full Outer Join ...|  4.225284|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Full Outer Join ...| 2.2428675|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation|         Running Sum|0.79645514|\n PySpark|        1|dataset_10MB|        1|    2|    Mixed Operation| Pivot 10 Rows an...| 2.2178774|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Inner Join 5 Col...|   1.35811|\n PySpark|        1|dataset_10MB|        1|    2|    Mixed Operation| Pivot 5 Rows and...| 1.8955848|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Inner Join 3 Col...| 3.4886348|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Sorting Desc 5 c...|0.89682007|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Left Outer Join ...| 3.2583692|\n PySpark|        1|dataset_10MB|        1|    2|    Mixed Operation| Pivot 1 Rows and...| 1.1653931|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Sorting Asc 10 c...| 1.0151405|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Split 1 Column i...| 0.9321842|\n PySpark|        1|dataset_10MB|        1|    2|Aggregate Operation|  GroupBy 10 columns|  5.738126|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Left Outer Join ...| 1.2920384|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Sorting Desc 1 c...| 0.8914995|\n PySpark|        1|dataset_10MB|        1|    2|Aggregate Operation|    Ranking by Group| 0.8991511|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation|         Shift (Lag)| 0.8548627|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation| Writing 1000 new...| 0.5800636|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation| Writing 10000 ne...| 0.7193711|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Left Outer Join ...| 1.1501529|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Split 1 Column i...|0.80199933|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Sorting Asc 1 co...| 0.9612789|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Merge 5 columns ...| 0.6563339|\n PySpark|        1|dataset_10MB|        1|    2|Aggregate Operation|   GroupBy 5 columns| 5.0338516|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Merge 2 columns ...| 0.6103804|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation| Writing 100 new ...| 0.5064764|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Merge 10 columns...| 0.6266656|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Mathematical Ope...|0.26103616|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Full Outer Join ...| 2.1501546|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation|              Filter| 0.3344679|\n PySpark|        1|dataset_10MB|        1|    2|Aggregate Operation|    GroupBy 1 column| 3.6146939|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Inner Join 10 Co...| 1.2582943|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Sorting Asc 5 co...| 0.8907826|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation|     Filter Reg Ex 1|0.33460474|\n PySpark|        1|dataset_10MB|        1|    2|   Column Operation| Sorting Desc 10 ...|0.87929964|\n PySpark|        1|dataset_10MB|        1|    2|      Row Operation|     Filter Reg Ex 2|0.32523847|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Inner Join 3 Col...| 3.4015462|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Left Outer Join ...| 1.2624812|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation| Writing 1000 new...| 0.5869286|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Sorting Desc 5 c...| 1.0035172|\n PySpark|        1|dataset_10MB|        1|    3|Aggregate Operation|   GroupBy 5 columns| 5.1428666|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Full Outer Join ...|  2.519245|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation|     Filter Reg Ex 2| 0.2863269|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Inner Join 10 Co...| 1.2024479|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation|     Filter Reg Ex 1| 0.3069811|\n PySpark|        1|dataset_10MB|        1|    3|    Mixed Operation| Pivot 1 Rows and...| 1.2954917|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Left Outer Join ...| 1.1487334|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Sorting Asc 1 co...| 1.6565611|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Sorting Asc 5 co...| 1.0202734|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Full Outer Join ...| 4.3072925|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Mathematical Ope...|0.26482368|\n PySpark|        1|dataset_10MB|        1|    3|Aggregate Operation|    Ranking by Group|0.84596014|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation| Writing 10000 ne...|0.77194834|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation|         Running Sum|  0.860163|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Split 1 Column i...|0.89331365|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Sorting Desc 1 c...|0.92180324|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Sorting Asc 10 c...| 1.1140742|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Merge 5 columns ...| 0.6029544|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation|         Shift (Lag)| 0.8132427|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Split 1 Column i...| 0.7445142|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation| Writing 100 new ...| 0.5641351|\n PySpark|        1|dataset_10MB|        1|    3|Aggregate Operation|    GroupBy 1 column| 3.4074922|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Left Outer Join ...| 3.5086577|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Sorting Desc 10 ...| 0.8550832|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Merge 10 columns...|0.70695066|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Merge 2 columns ...| 0.6230869|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Inner Join 5 Col...| 1.0810988|\n PySpark|        1|dataset_10MB|        1|    3|    Mixed Operation| Pivot 5 Rows and...| 1.9555938|\n PySpark|        1|dataset_10MB|        1|    3|Aggregate Operation|  GroupBy 10 columns| 5.2724624|\n PySpark|        1|dataset_10MB|        1|    3|      Row Operation|              Filter|0.39787197|\n PySpark|        1|dataset_10MB|        1|    3|    Mixed Operation| Pivot 10 Rows an...| 2.1125922|\n PySpark|        1|dataset_10MB|        1|    3|   Column Operation| Full Outer Join ...| 1.9748278|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Sorting Desc 1 c...|0.92934656|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Sorting Asc 10 c...| 0.9759164|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Split 1 Column i...|0.76928806|\n PySpark|        1|dataset_10MB|        1|    4|    Mixed Operation| Pivot 1 Rows and...| 1.1505036|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Full Outer Join ...| 2.3115518|\n PySpark|        1|dataset_10MB|        1|    4|    Mixed Operation| Pivot 5 Rows and...| 2.0227838|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation|     Filter Reg Ex 2|0.37258506|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Merge 10 columns...|0.56920815|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Left Outer Join ...| 1.3247454|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Left Outer Join ...| 3.0577583|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Inner Join 3 Col...| 3.2946026|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Full Outer Join ...| 3.9321613|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation|         Shift (Lag)| 0.8378508|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Full Outer Join ...|   2.14873|\n PySpark|        1|dataset_10MB|        1|    4|    Mixed Operation| Pivot 10 Rows an...|  2.093944|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Sorting Desc 5 c...| 0.9240079|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation|              Filter|0.37750268|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation| Writing 10000 ne...| 0.7766378|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation|     Filter Reg Ex 1|0.28437185|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation|         Running Sum| 0.8389113|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Merge 2 columns ...| 0.5808451|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation| Writing 1000 new...| 0.5189264|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Mathematical Ope...|0.28241038|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Sorting Asc 5 co...| 0.9064424|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Sorting Asc 1 co...|0.83735013|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Inner Join 10 Co...| 1.3113971|\n PySpark|        1|dataset_10MB|        1|    4|Aggregate Operation|   GroupBy 5 columns|  4.935361|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Merge 5 columns ...|0.68835497|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Split 1 Column i...| 0.6450062|\n PySpark|        1|dataset_10MB|        1|    4|Aggregate Operation|    Ranking by Group|  0.867501|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Sorting Desc 10 ...| 1.0654716|\n PySpark|        1|dataset_10MB|        1|    4|Aggregate Operation|    GroupBy 1 column| 3.4662807|\n PySpark|        1|dataset_10MB|        1|    4|Aggregate Operation|  GroupBy 10 columns|      5.25|\n PySpark|        1|dataset_10MB|        1|    4|      Row Operation| Writing 100 new ...| 0.5392411|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Inner Join 5 Col...| 1.2605605|\n PySpark|        1|dataset_10MB|        1|    4|   Column Operation| Left Outer Join ...| 1.3273621|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Full Outer Join ...| 2.0896392|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Sorting Desc 1 c...|  0.949033|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Merge 10 columns...|0.71182275|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Left Outer Join ...| 1.2202611|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Merge 2 columns ...| 0.6013341|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Sorting Asc 1 co...| 0.8608546|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Full Outer Join ...|  4.008902|\n PySpark|        1|dataset_10MB|        1|    5|Aggregate Operation|    Ranking by Group|0.78735995|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Sorting Desc 10 ...| 1.0547519|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Inner Join 3 Col...| 3.4132712|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Mathematical Ope...|0.27145433|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation|         Running Sum| 0.8552227|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Inner Join 10 Co...| 1.2987487|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Left Outer Join ...| 1.2465866|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation| Writing 100 new ...| 0.5408542|\n PySpark|        1|dataset_10MB|        1|    5|Aggregate Operation|  GroupBy 10 columns|  5.637368|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation|     Filter Reg Ex 1| 0.3668623|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Sorting Asc 5 co...| 0.9144366|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Sorting Desc 5 c...| 0.9021027|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation| Writing 10000 ne...| 0.7640469|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Merge 5 columns ...| 0.6007817|\n PySpark|        1|dataset_10MB|        1|    5|Aggregate Operation|   GroupBy 5 columns| 5.1262383|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation|              Filter|0.35124707|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation|     Filter Reg Ex 2|0.28192925|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Full Outer Join ...|  4.310137|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Left Outer Join ...| 6.3222837|\n PySpark|        1|dataset_10MB|        1|    5|    Mixed Operation| Pivot 5 Rows and...| 2.5292222|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Sorting Asc 10 c...| 1.1927989|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation|         Shift (Lag)| 0.8092706|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Split 1 Column i...| 1.0137773|\n PySpark|        1|dataset_10MB|        1|    5|      Row Operation| Writing 1000 new...|0.59639096|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Split 1 Column i...| 0.7778039|\n PySpark|        1|dataset_10MB|        1|    5|   Column Operation| Inner Join 5 Col...| 1.4865301|\n PySpark|        1|dataset_10MB|        1|    5|    Mixed Operation| Pivot 10 Rows an...|  2.309037|\n PySpark|        1|dataset_10MB|        1|    5|    Mixed Operation| Pivot 1 Rows and...| 1.2641964|\n PySpark|        1|dataset_10MB|        1|    5|Aggregate Operation|    GroupBy 1 column| 3.4417589|\n PySpark|        1|dataset_10MB|        1|    6|    Mixed Operation| Pivot 10 Rows an...|  2.152759|\n PySpark|        1|dataset_10MB|        1|    6|    Mixed Operation| Pivot 1 Rows and...| 1.0759017|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation| Writing 10000 ne...| 0.8055277|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation|         Running Sum|  0.924618|\n PySpark|        1|dataset_10MB|        1|    6|Aggregate Operation|    Ranking by Group| 0.8395059|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Left Outer Join ...| 3.4698465|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Full Outer Join ...| 1.9673443|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Left Outer Join ...|  1.321182|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Sorting Asc 1 co...| 0.8179891|\n PySpark|        1|dataset_10MB|        1|    6|Aggregate Operation|  GroupBy 10 columns|  4.880507|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Inner Join 3 Col...| 2.9312174|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Merge 5 columns ...| 0.6189766|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Left Outer Join ...|  1.143323|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Split 1 Column i...| 0.7566521|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Merge 2 columns ...|0.59566283|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Split 1 Column i...| 0.7790854|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation| Writing 100 new ...| 0.6374414|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation|         Shift (Lag)| 0.7714083|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Sorting Asc 10 c...|  1.149148|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Sorting Desc 1 c...| 0.8923676|\n PySpark|        1|dataset_10MB|        1|    6|Aggregate Operation|    GroupBy 1 column| 2.8746164|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation|              Filter|0.37866092|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Full Outer Join ...|  3.543535|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Inner Join 10 Co...| 1.1347725|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation|     Filter Reg Ex 2| 0.3289342|\n PySpark|        1|dataset_10MB|        1|    6|    Mixed Operation| Pivot 5 Rows and...|  1.837985|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation|     Filter Reg Ex 1| 0.2614951|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Sorting Desc 10 ...| 0.9764159|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Mathematical Ope...|0.28274512|\n PySpark|        1|dataset_10MB|        1|    6|Aggregate Operation|   GroupBy 5 columns| 5.6010466|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Sorting Desc 5 c...| 0.8877795|\n PySpark|        1|dataset_10MB|        1|    6|      Row Operation| Writing 1000 new...| 0.5152416|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Sorting Asc 5 co...|0.86731863|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Merge 10 columns...|0.69424796|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Full Outer Join ...| 1.6310608|\n PySpark|        1|dataset_10MB|        1|    6|   Column Operation| Inner Join 5 Col...| 1.0997069|\n+--------+---------+------------+---------+-----+-------------------+--------------------+----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Displays all files in a location\n# https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\ndisplay(dbutils.fs.ls(\"dbfs:/mnt/blob/\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/datasets/</td><td>datasets/</td><td>0</td></tr><tr><td>dbfs:/mnt/blob/timelogs/</td><td>timelogs/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/mnt/blob/datasets\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB.csv</td><td>dataset_100MB.csv</td><td>121258369</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_100.csv</td><td>dataset_100MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_1000.csv</td><td>dataset_100MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_100MB_add_10000.csv</td><td>dataset_100MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB.csv</td><td>dataset_10MB.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_100.csv</td><td>dataset_10MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_1000.csv</td><td>dataset_10MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_10MB_add_10000.csv</td><td>dataset_10MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB.csv</td><td>dataset_200MB.csv</td><td>241022871</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_100.csv</td><td>dataset_200MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_1000.csv</td><td>dataset_200MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_200MB_add_10000.csv</td><td>dataset_200MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB.csv</td><td>dataset_300MB.csv</td><td>359945718</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_100.csv</td><td>dataset_300MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_1000.csv</td><td>dataset_300MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_300MB_add_10000.csv</td><td>dataset_300MB_add_10000.csv</td><td>12036588</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB.csv</td><td>dataset_500MB.csv</td><td>604690884</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_100.csv</td><td>dataset_500MB_add_100.csv</td><td>121049</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_1000.csv</td><td>dataset_500MB_add_1000.csv</td><td>1208064</td></tr><tr><td>dbfs:/mnt/blob/datasets/dataset_500MB_add_10000.csv</td><td>dataset_500MB_add_10000.csv</td><td>12036588</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/mnt/blob/timelogs\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_100MB_20190407_0526.csv</td><td>PySpark_dataset_100MB_20190407_0526.csv</td><td>15839</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_10MB_20190407_0546.csv</td><td>PySpark_dataset_10MB_20190407_0546.csv</td><td>17093</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_200MB_20190406_2220.csv</td><td>PySpark_dataset_200MB_20190406_2220.csv</td><td>4316</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_200MB_20190407_0456.csv</td><td>PySpark_dataset_200MB_20190407_0456.csv</td><td>12853</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_300MB_20190407_0413.csv</td><td>PySpark_dataset_300MB_20190407_0413.csv</td><td>12920</td></tr><tr><td>dbfs:/mnt/blob/timelogs/PySpark_dataset_500MB_20190407_0246.csv</td><td>PySpark_dataset_500MB_20190407_0246.csv</td><td>12927</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_100MB_20190407_1236.csv</td><td>time_Scala_Random_dataset_100MB_20190407_1236.csv</td><td>14281</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_10MB_20190406_0941.csv</td><td>time_Scala_Random_dataset_10MB_20190406_0941.csv</td><td>5135</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_10MB_20190407_1227.csv</td><td>time_Scala_Random_dataset_10MB_20190407_1227.csv</td><td>15402</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190406_0952.csv</td><td>time_Scala_Random_dataset_200MB_20190406_0952.csv</td><td>291</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190406_0957.csv</td><td>time_Scala_Random_dataset_200MB_20190406_0957.csv</td><td>564</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190406_1006.csv</td><td>time_Scala_Random_dataset_200MB_20190406_1006.csv</td><td>514</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190406_1121.csv</td><td>time_Scala_Random_dataset_200MB_20190406_1121.csv</td><td>744</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190406_1133.csv</td><td>time_Scala_Random_dataset_200MB_20190406_1133.csv</td><td>55</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190407_0225.csv</td><td>time_Scala_Random_dataset_200MB_20190407_0225.csv</td><td>11587</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_200MB_20190407_1213.csv</td><td>time_Scala_Random_dataset_200MB_20190407_1213.csv</td><td>3866</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_300MB_20190407_0155.csv</td><td>time_Scala_Random_dataset_300MB_20190407_0155.csv</td><td>11597</td></tr><tr><td>dbfs:/mnt/blob/timelogs/time_Scala_Random_dataset_500MB_20190407_1257.csv</td><td>time_Scala_Random_dataset_500MB_20190407_1257.csv</td><td>11630</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# DELETING FILES (USE WITH CARE)\n#dbutils.fs.rm(\"/mnt/blob/timelogs/PySpark_dataset_500MB_20190323_2323.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Moving a File\n# https://forums.databricks.com/questions/14312/how-to-move-files-of-same-extension-in-databricks.html\n# dbutils.fs.mv(\"dbfs:/mnt/blob/add_100.csv\", \"dbfs:/mnt/blob/datasets/.\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["print(\"Open with\")\nprint(\"https://dbc-b260fb76-33af.cloud.databricks.com/dbfs\" + logfilename + \"?o=6744756749927366\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Open with\nhttps://dbc-b260fb76-33af.cloud.databricks.com/dbfs/mnt/blob/timelogs/PySpark_dataset_10MB_20190407_0546.csv?o=6744756749927366\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"Project_PySpark_v2","notebookId":3331007012568436},"nbformat":4,"nbformat_minor":0}
